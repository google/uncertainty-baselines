{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZR1tYwA2o8C"
      },
      "source": [
        "# RDL Big Paper Plots\n",
        "\n",
        "*Licensed under the Apache License, Version 2.0.*\n",
        "\n",
        "To run this in a public Colab, change the GitHub link: replace github.com with [githubtocolab.com](http://githubtocolab.com).\n",
        "\n",
        "This colab loads raw measurements from disk and analyzes the results.\n",
        "\n",
        "## Choosing optimal hyperparameters\n",
        "We automatically detect hyperparameter sweeps by selecting fields that don't correspond to dataset metrics but that have more than one chosen value. We choose the hyperparameters that achieve the best according a given metric (see `dataset_metric`) after averaging over random seeds. For example, if the model is trained on CIFAR-10, we use CIFAR-10's validation loss.\n",
        "\n",
        "## Plots\n",
        "All plots report the performance of a given model according to its optimal hyperparameters chosen above. When there are runs with multiple seeds, we show the mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3lFstNIFvry"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qpT4rtuEN14"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4JuGWk_t6FW"
      },
      "outputs": [],
      "source": [
        "#@title Choosing optimal hyperparameters\n",
        "\n",
        "# The finetuning deterministic jobs use a fixed random seed but different\n",
        "# upstream checkpoints, which themselves correspond to different random seeds. \n",
        "# In this case, we thus marginalize over upstream checkpoints\n",
        "# (`config.model_init`) rather than the random seed.\n",
        "\n",
        "def is_hparam(col):\n",
        "  return col.startswith('config.') or col in ['learning_rate', 'model']\n",
        "\n",
        "\n",
        "def _get_hparams(df, additional_marginalization_hparams: str):\n",
        "  marginalization_hparams = ['config.seed']\n",
        "  if additional_marginalization_hparams:\n",
        "    marginalization_hparams.append(additional_marginalization_hparams)\n",
        "\n",
        "  candidates = [c for c in df.columns if is_hparam(c)]\n",
        "  return [\n",
        "      c for c in candidates\n",
        "      if len(df[c].unique()) \u003e 1 and c not in marginalization_hparams\n",
        "  ]\n",
        "\n",
        "\n",
        "def _get_best_hparams(df,\n",
        "                      dataset_metric: Dict[str, str],\n",
        "                      additional_marginalization_hparams: str):\n",
        "  hps = _get_hparams(df, additional_marginalization_hparams)\n",
        "  ds = df['config.dataset'].iloc[0]\n",
        "  model = df['model'].iloc[0]\n",
        "  metric = dataset_metric[ds]\n",
        "\n",
        "  print(f'For {model} on {ds}, found {len(hps)} hparams: {hps}')\n",
        "  hps = hps + ['model']  # To ensure we don't groupby an empty list.\n",
        "  aggregated_loss = df.groupby(hps)[metric].agg('mean').reset_index()\n",
        "\n",
        "  if metric == 'in_domain_validation/auroc':\n",
        "    return aggregated_loss.loc[aggregated_loss[metric].idxmax()][hps]\n",
        "\n",
        "  return aggregated_loss.loc[aggregated_loss[metric].idxmin()][hps]\n",
        "\n",
        "\n",
        "def _get_optimal_results(df: pd.DataFrame,\n",
        "                         dataset_metric: Dict[str, str],\n",
        "                         additional_marginalization_hparams: str):\n",
        "  df = df.copy()\n",
        "  best_hps = _get_best_hparams(\n",
        "      df,\n",
        "      dataset_metric=dataset_metric,\n",
        "      additional_marginalization_hparams=additional_marginalization_hparams)\n",
        "  print(f'  Best hparams: {dict(best_hps)}')\n",
        "  for k, v in best_hps.items():\n",
        "    df = df[df[k] == v]\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_optimal_results(measurements: Dict[str, pd.DataFrame],\n",
        "                        dataset_metric: Dict[str, str]):\n",
        "  \"\"\"Returns a dataframe, typically with one result per model type.\n",
        "\n",
        "  A model type may have multiple results that will be averaged over when\n",
        "  plotting (e.g., random seeds).\n",
        "\n",
        "  Args:\n",
        "    measurements: Dictionary of dataframes to obtain best results for.\n",
        "    dataset_metric: Each dataset's metric to tune for, in the format\n",
        "      `{dataset: metric}`.\n",
        "  \"\"\"\n",
        "  results = []\n",
        "  for k, v in measurements.items():\n",
        "    additional_marginalization_hparams = None\n",
        "    if k in ('Det', 'Det I21K', 'DE'):\n",
        "      additional_marginalization_hparams = 'config.model_init'\n",
        "    for ds in v['config.dataset'].unique():\n",
        "      df = v[v['config.dataset'] == ds]\n",
        "      try:\n",
        "        results.append(\n",
        "            _get_optimal_results(\n",
        "                df,\n",
        "                dataset_metric=dataset_metric,\n",
        "                additional_marginalization_hparams=additional_marginalization_hparams\n",
        "            ))\n",
        "      except KeyError:\n",
        "        print(f'Could not get optimal results for {k}, {ds}.')\n",
        "    print()\n",
        "  return pd.concat(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMdPlsI12gec"
      },
      "outputs": [],
      "source": [
        "#@title Obtain reliability score\n",
        "\n",
        "SPLIT_METRICS = ['loss', 'prec@1', 'ece', 'calib_auc']\n",
        "IND_METRICS = [f'test_{m}' for m in SPLIT_METRICS]\n",
        "FEWSHOT_DATASETS = ['imagenet', 'pets', 'birds', 'col_hist', 'cifar100', 'caltech', 'cars', 'dtd', 'uc_merced']\n",
        "FEWSHOT_METRICS = [\n",
        "    f'z/{ds}_{f}shot' for (ds, f) in itertools.product(\n",
        "        FEWSHOT_DATASETS,\n",
        "        [1, 5, 10, 25])\n",
        "]\n",
        "OOD_METRICS = [\n",
        "    f'{ds}_{m}' for (ds, m) in itertools.product(\n",
        "        ['cifar_10h', 'imagenet_real'],\n",
        "        ['loss', 'prec@1', 'ece', 'calib_auc'])\n",
        "]\n",
        "OOD_DETECTION_METRICS = [\n",
        "    f'ood_{ds}_{method}_auroc'\n",
        "    for (ds, method) in itertools.product(\n",
        "        ['cifar10', 'cifar100', 'svhn_cropped', 'places365_small'],\n",
        "        # We use just MSP following Jie's recommendation.\n",
        "        ['msp'])\n",
        "        # ['entropy', 'maha', 'msp', 'rmaha'])\n",
        "]\n",
        "COMPUTE_METRICS = ['exaflops', 'tpu_days', 'gflops', 'ms_step']\n",
        "RETINA_METRICS = ['accuracy', 'negative_log_likelihood', 'ece', 'retention_auroc_auc', 'retention_accuracy_auc']\n",
        "RETINA_METRICS = [f'{prefix}/{metric}' for metric in RETINA_METRICS for prefix in ['in_domain_test', 'ood_test']]\n",
        "METRICS = IND_METRICS + FEWSHOT_METRICS + OOD_METRICS + OOD_DETECTION_METRICS + COMPUTE_METRICS + RETINA_METRICS\n",
        "CATEGORIES = {\n",
        "    'prediction': [\n",
        "        'test_loss',\n",
        "        'test_prec@1',\n",
        "        'cifar_10h_loss',\n",
        "        'cifar_10h_prec@1',\n",
        "        'imagenet_real_loss',\n",
        "        'imagenet_real_prec@1',\n",
        "        # RETINA\n",
        "        'in_domain_test/negative_log_likelihood',\n",
        "        'ood_test/negative_log_likelihood',\n",
        "        'in_domain_test/accuracy',\n",
        "        'ood_test/accuracy',\n",
        "    ],\n",
        "    'uncertainty': [\n",
        "        'cifar_10h_calib_auc',\n",
        "        'cifar_10h_ece',\n",
        "        'imagenet_real_calib_auc',\n",
        "        'imagenet_real_ece',\n",
        "        'ood_cifar100_msp_auroc',\n",
        "        'ood_cifar10_msp_auroc',\n",
        "        'ood_places365_small_msp_auroc',\n",
        "        'ood_svhn_cropped_msp_auroc',\n",
        "        'test_calib_auc',\n",
        "        'test_ece',\n",
        "        # RETINA\n",
        "        'in_domain_test/ece',\n",
        "        'ood_test/ece',\n",
        "        'in_domain_test/retention_auroc_auc',\n",
        "        'ood_test/retention_auroc_auc',\n",
        "        'in_domain_test/retention_accuracy_auc',\n",
        "        'ood_test/retention_accuracy_auc'\n",
        "    ],\n",
        "    'adaptation': [\n",
        "        '10shot_prec@1',\n",
        "        '25shot_prec@1',\n",
        "        '5shot_prec@1',\n",
        "    ],\n",
        "}\n",
        "RETINA_NLL_METRICS = [\n",
        "  metric for metric in RETINA_METRICS if 'negative_log_likelihood' in metric]\n",
        "\n",
        "DATASET_CLASSES = {\n",
        "      'cifar10': 10,\n",
        "      'cifar100': 100,\n",
        "      'imagenet2012': 1000,\n",
        "      'imagenet21k': 21841,\n",
        "      'jft/entity:1.0.0': 18291,\n",
        "      'retina_country': 2,\n",
        "      'retina_severity': 2,\n",
        "}\n",
        "\n",
        "def preprocess(df,\n",
        "               split_metrics=SPLIT_METRICS,\n",
        "               metrics=METRICS,\n",
        "               compute_metrics=COMPUTE_METRICS,\n",
        "               fewshot_datasets=FEWSHOT_DATASETS):\n",
        "  df = df.copy()\n",
        "  df = df.groupby(['model', 'config.dataset']).agg('mean').reset_index()\n",
        "  # Set JFT/I21K upstream #s to the test set reporting since we use them that\n",
        "  # way.\n",
        "  for m in split_metrics:\n",
        "    df.loc[df['config.dataset'] == 'jft/entity:1.0.0', f'test_{m}'] = df.loc[\n",
        "        df['config.dataset'] == 'jft/entity:1.0.0', f'val_{m}']\n",
        "    df.loc[df['config.dataset'] == 'imagenet21k', f'test_{m}'] = df.loc[\n",
        "        df['config.dataset'] == 'imagenet21k', f'val_{m}']\n",
        "\n",
        "  cols = ['model', 'config.dataset'] + metrics\n",
        "  df = df[cols].copy()\n",
        "  df = df.pivot(index='model', columns='config.dataset', values=metrics)\n",
        "\n",
        "  # Drop columns with all NaNs, e.g., ECE for JFT. They aren't measured.\n",
        "  df = df.dropna(axis=1, how='all')\n",
        "\n",
        "  # Set few-shot imagenet metrics under a distinct dataset so later, we can\n",
        "  # aggregate over few-shot metrics while excluding their original\n",
        "  # config.dataset (the upstream dataset).\n",
        "  for ds in fewshot_datasets:\n",
        "    for f in [1, 5, 10, 25]:\n",
        "      df[f'{f}shot_prec@1', f'few-shot {ds}'] = df[f'z/{ds}_{f}shot'].mean(axis=1)\n",
        "      del df[f'z/{ds}_{f}shot']\n",
        "  # Do same for compute and only keep upstream compute metrics.\n",
        "  for metric in compute_metrics:\n",
        "    for ds in df[metric]:\n",
        "      if ds == 'imagenet21k':\n",
        "        df[metric, 'compute'] = df[metric, ds]\n",
        "      elif ds == 'jft/entity:1.0.0':\n",
        "        df[metric, 'compute'] = np.where(df[metric, 'compute'].isnull(), df[metric, ds], df[metric, 'compute'])\n",
        "      del df[metric, ds]\n",
        "  return df\n",
        "\n",
        "def compute_score(df, datasets=None, categories=CATEGORIES):\n",
        "  \"\"\"Compute aggregate score across metrics and per-category scores.\"\"\"\n",
        "  df = df.copy()\n",
        "\n",
        "  # Scale all metrics in range [0.0, 1.0], and where higher is better.\n",
        "  for column in df.columns:\n",
        "    metric, dataset = column\n",
        "    if 'ece' in metric:\n",
        "      df[column] = 1. - df[column]\n",
        "    if 'dataset' == 'compute':\n",
        "      del df[column]\n",
        "  # Remove 1-shot for now as its #s are unreliable due to high variance.\n",
        "  del df['1shot_prec@1']\n",
        "\n",
        "  for metric, dataset in df[['test_loss', 'cifar_10h_loss', 'imagenet_real_loss'] + RETINA_NLL_METRICS]:\n",
        "    # Rescale NLL under its bound [0.0, uniform entropy]. Technically I21K \u0026\n",
        "    # JFT's uniform entropy should be computed on multiclass sigmoid NLL, but\n",
        "    # unlike categorical uniform, multiclass sigmoid uniform is so large it's\n",
        "    # meaningless as a bound.\n",
        "    num_classes = DATASET_CLASSES[dataset]\n",
        "    p = 1./num_classes\n",
        "    max_value = -num_classes * p * np.log(p)\n",
        "    df.loc[:, (metric, dataset)] = 1. - df[metric][dataset] / max_value\n",
        "\n",
        "  # Flatten multiindexes.\n",
        "  df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
        "  if datasets is not None:\n",
        "    metrics = [m for m in df.columns if any(d == m.split('_')[-1] for d in datasets)]\n",
        "    df = df[metrics]\n",
        "  # Compute the score only for models that have filled in all metrics.\n",
        "  subset_df = df.dropna(how='any')\n",
        "  score = subset_df.mean(axis=1) * 100.0\n",
        "  df_scores = score.sort_values(ascending=False).to_frame(name='score')\n",
        "  for key, value in categories.items():\n",
        "    metrics = [m for m in df.columns if '_'.join(m.split('_')[:-1]) in value]\n",
        "    subset_df = df[metrics]\n",
        "    subset_df = subset_df.dropna(how='any')\n",
        "    score = subset_df.mean(axis=1) * 100.0\n",
        "    df_scores[f'score_{key}'] = score\n",
        "\n",
        "  return df_scores\n",
        "\n",
        "def compute_relative_score_and_ranks(\n",
        "    df, datasets=None, categories=CATEGORIES, baseline_model='Det'):\n",
        "  \"\"\"Compute aggregate score across metrics and per-category scores.\"\"\"\n",
        "  df = df.copy()\n",
        "\n",
        "  # Scale all metrics in range [0.0, 1.0], and where higher is better.\n",
        "  for column in df.columns:\n",
        "    metric, dataset = column\n",
        "    if 'ece' in metric:\n",
        "      df[column] = 1. - df[column]\n",
        "    if 'dataset' == 'compute':\n",
        "      del df[column]\n",
        "  # Remove 1-shot for now as its #s are unreliable due to high variance.\n",
        "  del df['1shot_prec@1']\n",
        "\n",
        "  for metric, dataset in df[['test_loss', 'cifar_10h_loss', 'imagenet_real_loss'] + RETINA_NLL_METRICS]:\n",
        "    # Rescale NLL under its bound [0.0, uniform entropy]. Technically I21K \u0026\n",
        "    # JFT's uniform entropy should be computed on multiclass sigmoid NLL, but\n",
        "    # unlike categorical uniform, multiclass sigmoid uniform is so large it's\n",
        "    # meaningless as a bound.\n",
        "    num_classes = DATASET_CLASSES[dataset]\n",
        "    p = 1./num_classes\n",
        "    max_value = -num_classes * p * np.log(p)\n",
        "    df.loc[:, (metric, dataset)] = 1. - df[metric][dataset]# / max_value\n",
        "\n",
        "  # Flatten multiindexes.\n",
        "  df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
        "  if datasets is not None:\n",
        "    metrics = [m for m in df.columns if any(d == m.split('_')[-1] for d in datasets)]\n",
        "    df = df[metrics]\n",
        "  # Compute the score only for models that have filled in all metrics.\n",
        "  subset_df = df.dropna(how='any')\n",
        "  baseline = subset_df.loc[baseline_model, :].to_numpy()[None, :]\n",
        "  score = subset_df.div(baseline, axis=1).mean(axis=1)\n",
        "  df_scores = score.sort_values(ascending=False).to_frame(name='rel_score')\n",
        "  df_ranks = subset_df.rank(axis=0, ascending=False)\n",
        "\n",
        "  ranks_by_category = {}\n",
        "  for key, value in categories.items():\n",
        "    metrics = [m for m in df.columns if '_'.join(m.split('_')[:-1]) in value]\n",
        "    subset_df = df[metrics]\n",
        "    subset_df = subset_df.dropna(how='any')\n",
        "    baseline = subset_df.loc['Det', :].to_numpy()[None, :]\n",
        "    subset_df = subset_df.div(baseline, axis=1)\n",
        "    rank_df = subset_df.rank(axis=0, ascending=False)\n",
        "    ranks_by_category[key] = rank_df\n",
        "    subset_np = subset_df.to_numpy()\n",
        "    winners = np.argmax(subset_np, axis=0).astype(np.int64)\n",
        "    wincount = np.bincount(winners.flatten(),\n",
        "                           minlength=subset_np.shape[0])\n",
        "    win_df = pd.DataFrame(data=wincount[:, None],\n",
        "                          index=subset_df.index)\n",
        "    score = subset_df.mean(axis=1)\n",
        "    df_scores[f'rel_score_{key}'] = score\n",
        "    df_scores[f'#_best_{key}'] = win_df\n",
        "    df_scores[f'mean_rank_{key}'] = rank_df.mean(axis=1)\n",
        "\n",
        "  return df_scores, df_ranks, ranks_by_category\n",
        "\n",
        "def pprint(df, models=None, exclude_models=None):\n",
        "  \"\"\"Pretty print dataframe.\n",
        "\n",
        "  Args:\n",
        "    df: Dataframe.\n",
        "    models: Optional list of models to only show. Useful for comparing specific\n",
        "      models to see which performs better (highlighted cells).\n",
        "    exclude_models: Optional list of models to exclude.\n",
        "  \"\"\"\n",
        "  def _rename(m):\n",
        "    m = m.replace('cifar_10h', 'cifar10h')\n",
        "    m = m.replace('places365_small', 'places365')\n",
        "    m = m.replace('_', ' ')\n",
        "    m = m.replace('cropped ', '')\n",
        "    m = m.replace('ood', '')\n",
        "    m = m.replace('ece', 'ECE')\n",
        "    m = m.replace('auc', 'AUC')\n",
        "    m = m.replace('auroc', 'AUROC')\n",
        "    m = m.replace('loss', 'NLL')\n",
        "    m = m.replace('negative log likelih', 'NLL')\n",
        "    return m\n",
        "  def _formatter(metric):\n",
        "    if any(x in metric for x in ['AUROC', 'AUC']):\n",
        "      return '{:.2f}'.format\n",
        "    elif any(x in metric for x in ['prec', 'ECE', 'accuracy']):\n",
        "      return lambda x: '{:.1f}%'.format(x * 100)\n",
        "    elif any(x in metric for x in ['score', 'exaflops', 'tpu days', 'gflops', 'ms step']):\n",
        "      return lambda x: '{:.1f}'.format(x)\n",
        "    elif 'NLL' in metric:\n",
        "      return '{:.3f}'.format\n",
        "    else:\n",
        "      return lambda x: x\n",
        "  def _highlight(data, color='#90EE90'):\n",
        "    attr = 'background-color: {}'.format(color)\n",
        "    data = data.replace('%','', regex=True).astype(float)\n",
        "    if any(x in data.name[1] for x in ['NLL', 'ECE']):\n",
        "      is_best = data == data.min()\n",
        "    elif any(x in data.name[1] for x in ['exaflops', 'tpu days', 'gflops', 'ms step']):\n",
        "      is_best = data == 'asdf'\n",
        "    else:\n",
        "      is_best = data == data.max()\n",
        "    return [attr if v else '' for v in is_best]\n",
        "\n",
        "  df = df.copy()\n",
        "  df = df.rename(columns=_rename)\n",
        "  for c in df:\n",
        "    df[c] = df[c].apply(_formatter(c[0]))\n",
        "\n",
        "  # Swap order of column's multiindex to be dataset first.\n",
        "  df.columns = df.columns.swaplevel(0, 1)\n",
        "  df = df.sort_index(axis=1, level=0)\n",
        "\n",
        "  df = df.T\n",
        "  if models is not None:\n",
        "    df = df[[c for c in df.columns if c in models]]\n",
        "  elif exclude_models is not None:\n",
        "    df = df[[c for c in df.columns if c not in exclude_models]]\n",
        "\n",
        "  return display.display(df.style.apply(_highlight, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBqSMsgf0U2k",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#@title RETINA\n",
        "REBUILD_RETINA_RESULTS_CACHE = False\n",
        "\n",
        "if REBUILD_RETINA_RESULTS_CACHE:\n",
        "  import os\n",
        "  os.system('pip install wandb')\n",
        "  import wandb\n",
        "\n",
        "# TODO(nband): add grid search results (currently random search).\n",
        "RETINA_SHIFT_AND_UQ_METHOD_TO_WANDB = {\n",
        "  ('aptos', 'deterministic'): 'vit32-finetune-aptos-deterministic-focused-3',\n",
        "  ('aptos', 'batchensemble'): 'vit32-finetune-aptos-batchensemble',\n",
        "  ('severity', 'deterministic'): 'vit32-finetune-severity-deterministic',\n",
        "  ('severity', 'batchensemble'): 'vit32-finetune-severity-batchensemble-focused-1'\n",
        "}\n",
        "\n",
        "RETINA_SHIFTS = ['aptos', 'severity']\n",
        "RETINA_UQ_METHODS = ['deterministic', 'batchensemble']\n",
        "RETINA_UQ_METHOD_TO_DF_NAME = {\n",
        "    'deterministic': 'Det I21K',\n",
        "    'batchensemble': 'BE L/32 (I21K)'\n",
        "}\n",
        "\n",
        "RETINA_SHIFT_TO_METRICS = {\n",
        "  'aptos': [\n",
        "    # In-Domain\n",
        "    'in_domain_test.in_domain_test/accuracy',\n",
        "    'in_domain_test.in_domain_test/negative_log_likelihood',\n",
        "    'in_domain_test.in_domain_test/ece',\n",
        "    'in_domain_test.in_domain_test/retention_auroc_auc',\n",
        "    # OOD\n",
        "    'ood_test.ood_test/accuracy',\n",
        "    'ood_test.ood_test/negative_log_likelihood',\n",
        "    'ood_test.ood_test/ece',\n",
        "    'ood_test.ood_test/retention_auroc_auc'\n",
        "  ],\n",
        "  'severity': [\n",
        "    # In-Domain\n",
        "    'in_domain_test.in_domain_test/accuracy',\n",
        "    'in_domain_test.in_domain_test/negative_log_likelihood',\n",
        "    'in_domain_test.in_domain_test/ece',\n",
        "    'in_domain_test.in_domain_test/retention_auroc_auc',\n",
        "    # OOD\n",
        "    'ood_test.ood_test/accuracy',\n",
        "    'ood_test.ood_test/negative_log_likelihood',\n",
        "    'ood_test.ood_test/ece',\n",
        "    'ood_test.ood_test/retention_accuracy_auc'\n",
        "  ]\n",
        "}\n",
        "RETINA_MODEL_SELECTION_METRIC = 'in_domain_validation.in_domain_validation/auroc'\n",
        "\n",
        "\n",
        "def select_top_model_from_project(project_name):\n",
        "  api = wandb.Api(timeout=100000000)\n",
        "  runs = api.runs(project_name)\n",
        "  print(f'Retrieved run results from Weights \u0026 Biases project {project_name}.')\n",
        "  sweep_history_df = []\n",
        "\n",
        "  # Get all full histories\n",
        "  for run in runs:\n",
        "    run_history_df = pd.DataFrame(run._full_history())\n",
        "\n",
        "    # Add run name\n",
        "    run_history_df['run_name'] = run.name\n",
        "    sweep_history_df.append(run_history_df)\n",
        "\n",
        "  sweep_history_df = pd.concat(sweep_history_df)\n",
        "  sweep_history_df.reset_index(inplace=True)\n",
        "\n",
        "  # Best performing step of the best performing model\n",
        "  top_idx = sweep_history_df[RETINA_MODEL_SELECTION_METRIC].idxmax()\n",
        "  return sweep_history_df.iloc[top_idx]\n",
        "\n",
        "\n",
        "def get_retina_i21k_results_df():\n",
        "  all_results_df = []\n",
        "  for shift in RETINA_SHIFTS:\n",
        "    for uq_method in RETINA_UQ_METHODS:\n",
        "      print(f'Retrieving results from shift {shift}, '\n",
        "            f'uncertainty quantification method {uq_method}.')\n",
        "      wandb_project = RETINA_SHIFT_AND_UQ_METHOD_TO_WANDB[(shift, uq_method)]\n",
        "      model_results = select_top_model_from_project(wandb_project)\n",
        "      result_df = model_results.to_frame().T\n",
        "      result_df['shift'] = shift\n",
        "      result_df['uq_method'] = uq_method\n",
        "      all_results_df.append(result_df)\n",
        "\n",
        "  return pd.concat(all_results_df)\n",
        "\n",
        "\n",
        "def add_retina_i21k_results(retina_results_df, preprocessed_df):\n",
        "  for shift in RETINA_SHIFTS:\n",
        "    for uq_method in RETINA_UQ_METHODS:\n",
        "      print(f'Adding results from shift {shift}, '\n",
        "            f'uncertainty quantification method {uq_method}.')\n",
        "      model_results = retina_results_df[\n",
        "        (retina_results_df['shift'] == shift) \u0026\n",
        "        (retina_results_df['uq_method'] == uq_method)]\n",
        "      n_results = len(model_results)\n",
        "      assert n_results == 1, f'Found {n_results} model results, expected 1.'\n",
        "      model_results = model_results.iloc[0]\n",
        "      metrics = RETINA_SHIFT_TO_METRICS[shift]\n",
        "      for metric in metrics:\n",
        "        df_metric_name = metric.split('.')[1]\n",
        "        per_metric_result = model_results[metric]\n",
        "        shift_df_name = shift_map[shift]\n",
        "        metric_shift_series = preprocessed_df[(\n",
        "          df_metric_name, f'retina_{shift_df_name}')]\n",
        "        metric_shift_series[\n",
        "          RETINA_UQ_METHOD_TO_DF_NAME[uq_method]] = per_metric_result\n",
        "        preprocessed_df[\n",
        "          (df_metric_name, f'retina_{shift_df_name}')] = metric_shift_series\n",
        "\n",
        "  return preprocessed_df\n",
        "\n",
        "if REBUILD_RETINA_RESULTS_CACHE:\n",
        "  # Retrieve RETINA I21K results from Weights \u0026 Biases\n",
        "  retina_i21k_results_df = get_retina_i21k_results_df()\n",
        "\n",
        "  # Store RETINA results in gs bucket\n",
        "  retina_ub_gs_file_path = 'gs://retina-i21k-results-df/retina-i21k-results.tsv'\n",
        "  with tf.io.gfile.GFile(retina_ub_gs_file_path, 'w') as f:\n",
        "    retina_i21k_results_df.to_csv(f, sep='\\t', index=None)\n",
        "\n",
        "# Split RETINA results into the two distributional shifts: Country Shift and\n",
        "# Severity Shift.\n",
        "\n",
        "SHIFT_MAP = {'aptos': 'country', 'severity': 'severity'}\n",
        "\n",
        "def add_distribution_shift_to_retina_ds_name(row):\n",
        "  dataset = str(row['config.dataset'])\n",
        "  if dataset == 'retina':\n",
        "    shift = SHIFT_MAP[str(row['config.distribution_shift'])]\n",
        "    row['config.dataset'] = f'{dataset}_{shift}'\n",
        "\n",
        "  return row\n",
        "\n",
        "def split_retina_results_by_shifts(raw_dict):\n",
        "  for model in raw_dict.keys():\n",
        "    raw_model_df = raw_dict[model]\n",
        "    if not len(raw_model_df[raw_model_df['config.dataset'] == 'retina']):\n",
        "        continue\n",
        "\n",
        "    print(f'Splitting RETINA results for model {model} by distribution shift.')\n",
        "\n",
        "    raw_model_df = raw_model_df.apply(\n",
        "        add_distribution_shift_to_retina_ds_name, axis='columns')\n",
        "    raw_dict[model] = raw_model_df\n",
        "\n",
        "  return raw_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GygKbZFjwiLV"
      },
      "source": [
        "## Load and preprocess measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFG3GNLoQN3a"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'marginalization-external-xgcp'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "measurements_path = '/tmp/big-paper-raw-measurements.pkl'\n",
        "!gsutil cp gs://ub-checkpoints/big-paper-raw-measurements.pkl {measurements_path}\n",
        "\n",
        "retina_path = '/tmp/retina-i21k-results.tsv'\n",
        "!gsutil cp gs://retina-i21k-results-df/retina-i21k-results.tsv {retina_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcnAzeyWsHu8"
      },
      "outputs": [],
      "source": [
        "with tf.io.gfile.GFile(measurements_path, 'rb') as f:\n",
        "  raw_measurements = pickle.load(f)\n",
        "\n",
        "with tf.io.gfile.GFile(retina_path, 'r') as f:\n",
        "  retina_i21k_results_df = pd.read_csv(f, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Bc5TFN3ZTk"
      },
      "outputs": [],
      "source": [
        "raw_measurements = split_retina_results_by_shifts(raw_measurements)\n",
        "dataset_metric = {\n",
        "    'cifar10': 'val_loss',\n",
        "    'cifar100': 'val_loss',\n",
        "    'imagenet2012': 'val_loss',\n",
        "    'imagenet21k': 'val_loss',\n",
        "    'jft/entity:1.0.0': 'val_loss',\n",
        "    'retina_country': 'in_domain_validation/auroc',\n",
        "    'retina_severity': 'in_domain_validation/auroc',\n",
        "}\n",
        "measurements = get_optimal_results(raw_measurements,\n",
        "                                   dataset_metric=dataset_metric)\n",
        "\n",
        "df = preprocess(measurements)\n",
        "df = add_retina_i21k_results(retina_results_df=retina_i21k_results_df,\n",
        "                             preprocessed_df=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuV4-5wzEQiE"
      },
      "source": [
        "## Compute reliability score and generate table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kRH3CQGwaQg"
      },
      "outputs": [],
      "source": [
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "    'retina_country',\n",
        "    'retina_severity',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in FEWSHOT_DATASETS]\n",
        "scores = compute_score(df, datasets=datasets)\n",
        "display.display(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQS1I1pRD9rm"
      },
      "outputs": [],
      "source": [
        "df_with_scores = df.copy()\n",
        "for column in scores.columns:\n",
        "  df_with_scores[column] = scores[column]\n",
        "\n",
        "pprint(\n",
        "    df_with_scores,\n",
        "    # models=['BE L/32', 'Det'],\n",
        "    # exclude_models=['DE', 'Det-\u003eDE'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RySd-77qMBe"
      },
      "outputs": [],
      "source": [
        "# Show a subset of the table's metrics + models\n",
        "metrics = ['score', 'score_prediction', 'score_uncertainty', 'score_adaptation', 'exaflops', 'test_loss', 'tpu_days']\n",
        "models = ['BE L/32', 'Det', 'GP', 'Het', 'BE L/32 (I21K)', 'Det I21K']\n",
        "pprint(df_with_scores.loc[models][metrics].rename(columns={'compute': 'z/compute'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBtgUevvEYrh"
      },
      "source": [
        "## Plot reliability score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsnOp4CVambb"
      },
      "outputs": [],
      "source": [
        "import colabtools.fileedit\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "matplotlib.rcParams['figure.dpi'] = 1000\n",
        "matplotlib.rcParams['lines.linewidth'] = 1.25\n",
        "# sns.set_style(\"whitegrid\")\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5qvJhS_YWwE"
      },
      "outputs": [],
      "source": [
        "def pareto_plot(df, x, y, ax, filename=None, **kwargs):\n",
        "  def is_on_pareto_front(p, points, higher_is_better):\n",
        "    if higher_is_better:\n",
        "      return len([\n",
        "          point for point in points if point[0] \u003c= p[0] and point[1] \u003e p[1]\n",
        "      ]) == 0\n",
        "    else:\n",
        "      return len([\n",
        "          point for point in points if point[0] \u003c= p[0] and point[1] \u003c p[1]\n",
        "      ]) == 0\n",
        "  def get_pareto_points(x, y, higher_is_better=True):\n",
        "    points = list(zip(x, y))\n",
        "    frontier = [\n",
        "        p for p in points if is_on_pareto_front(p, points, higher_is_better)\n",
        "    ]\n",
        "    return sorted(frontier, key=lambda x: x[0])\n",
        "  for model, point in df.iterrows():\n",
        "    ann = ax.annotate(\n",
        "        '  ' + model,\n",
        "        xy=(point[x], point[y]),\n",
        "        ha='left',\n",
        "        va='bottom',\n",
        "  )\n",
        "  sns.scatterplot(x=df[x], y=df[y], ax=ax)\n",
        "  pareto_frontier = get_pareto_points(df[x], df[y])\n",
        "  xx, yy = zip(*pareto_frontier)\n",
        "  sns.lineplot(x=xx, y=yy, linestyle='--', ax=ax)\n",
        "  ax.set(xscale='log', **kwargs)\n",
        "  if filename is not None:\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    colabtools.fileedit.download_file(filename)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10.0, 5.0))\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=ax,\n",
        "    y='score',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel='Compute (TPUv3 core days)',\n",
        "    ylabel='Reliability Score',\n",
        "    filename='reliability.png',\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(3.5 * 3, 3.5))\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=axes[0],\n",
        "    y='score_prediction',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel=None,\n",
        "    ylabel=None,\n",
        "    title='Reliability Score (Prediction)',\n",
        ")\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=axes[1],\n",
        "    y='score_uncertainty',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel=None,\n",
        "    ylabel=None,\n",
        "    title='Reliability Score (Uncertainty)',\n",
        ")\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=axes[2],\n",
        "    y='score_adaptation',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel=None,\n",
        "    ylabel=None,\n",
        "    title='Reliability Score (Adaptation)',\n",
        ")\n",
        "filename = 'reliability_components.png'\n",
        "plt.tight_layout()\n",
        "plt.savefig(filename)\n",
        "colabtools.fileedit.download_file(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj2HLvvlEg46"
      },
      "source": [
        "## Analyze correlation of metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIaSlPrx8JLB"
      },
      "outputs": [],
      "source": [
        "temp_df = preprocess(\n",
        "    measurements,\n",
        "    metrics=METRICS + ['training_loss', 'training_prec@1'])\n",
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in FEWSHOT_DATASETS]\n",
        "temp_scores = compute_score(temp_df, datasets=datasets)\n",
        "for column in temp_scores.columns:\n",
        "  temp_df[column] = temp_scores[column]\n",
        "\n",
        "# scores correlation matrix\n",
        "columns = ['score', 'score_prediction', 'score_uncertainty', 'score_adaptation']\n",
        "corr_matrix = temp_df[columns]\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "corr_matrix = corr_matrix.corr()\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# upstream test metrics\n",
        "metrics = ['score', 'score_prediction', 'score_uncertainty', 'score_adaptation']\n",
        "corr_matrix = temp_df.corr()[['test_loss', 'test_prec@1']].T.xs(\n",
        "    'jft/entity:1.0.0', level='config.dataset')\n",
        "corr_matrix = corr_matrix[metrics]\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# imagenet 10-shot. It doesn't correlate well with reliability, mostly due to\n",
        "# it not correlating well surprisingly on other few-shot tasks.\n",
        "corr_matrix = temp_df.corr()[['10shot_prec@1']].T.xs(\n",
        "    'few-shot imagenet', level='config.dataset')\n",
        "corr_matrix = corr_matrix[metrics]\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# downstream training loss. The correlation is not nearly as tight as on\n",
        "# upstream.\n",
        "corr_matrix = temp_df.corr()[['training_loss']].T\n",
        "corr_matrix = corr_matrix[metrics + ['test_loss']]\n",
        "corr_matrix = corr_matrix.drop(index=('training_loss', 'retina'))\n",
        "corr_matrix = corr_matrix.drop(index=('training_loss', 'imagenet21k'))\n",
        "corr_matrix = corr_matrix.drop(columns=('test_loss', 'imagenet21k'))\n",
        "# Display test loss only for training loss' same downstream dataset. Looking at\n",
        "# cifar10's train loss correlation with I1K's test loss isn't meaningful.\n",
        "test_loss = pd.Series(np.diag(corr_matrix['test_loss']),\n",
        "                      index=corr_matrix['test_loss'].index)\n",
        "corr_matrix = corr_matrix.drop(columns='test_loss')\n",
        "corr_matrix['test_loss'] = test_loss\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# Similar to old plot in go/rdl-big-meeting, even generalization gap decreases.\n",
        "# And downstream is not very indicative, but upstream is.\n",
        "temp_df2 = temp_df.copy()\n",
        "for d in temp_df2['test_loss'].columns:\n",
        "  temp_df2['reg_loss', d] = temp_df2['test_loss', d] - temp_df2['training_loss', d]\n",
        "\n",
        "corr_matrix = temp_df2.corr()[['reg_loss']].T\n",
        "corr_matrix = corr_matrix[metrics + ['training_loss']]\n",
        "corr_matrix = corr_matrix.drop(index=('reg_loss', 'imagenet21k'))\n",
        "display.display(corr_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_ShrtSoVQwh"
      },
      "outputs": [],
      "source": [
        "corr_matrix = temp_df.corr()[['test_loss', 'test_prec@1', 'training_loss']].T.xs('jft/entity:1.0.0', level='config.dataset')\n",
        "\n",
        "# Rename certain task metrics to be under their generic metric name. This way,\n",
        "# we can average values across that metric.\n",
        "corr_matrix.columns = corr_matrix.columns.values\n",
        "corr_matrix.columns = pd.MultiIndex.from_tuples(corr_matrix.rename(columns={\n",
        "    ('imagenet_real_calib_auc', 'imagenet2012'): ('test_calib_auc', 'imagenet_real'),\n",
        "    ('imagenet_real_ece', 'imagenet2012'): ('test_ece', 'imagenet_real'),\n",
        "    ('imagenet_real_loss', 'imagenet2012'): ('test_loss', 'imagenet_real'),\n",
        "    ('imagenet_real_prec@1', 'imagenet2012'): ('test_prec@1', 'imagenet_real'),\n",
        "    ('cifar_10h_calib_auc', 'cifar10'): ('test_calib_auc', 'cifar_10h'),\n",
        "    ('cifar_10h_ece', 'cifar10'): ('test_ece', 'cifar_10h'),\n",
        "    ('cifar_10h_loss', 'cifar10'): ('test_loss', 'cifar_10h'),\n",
        "    ('cifar_10h_prec@1', 'cifar10'): ('test_prec@1', 'cifar_10h'),\n",
        "    ('ood_cifar100_msp_auroc', 'cifar10'): ('msp_auroc', 'cifar10-\u003ecifar100'),\n",
        "    ('ood_cifar10_msp_auroc', 'cifar100'): ('msp_auroc', 'cifar100-\u003ecifar10'),\n",
        "    ('ood_places365_small_msp_auroc', 'imagenet2012'): ('msp_auroc', 'imagenet2012-\u003eplaces365'),\n",
        "    ('ood_svhn_cropped_msp_auroc', 'cifar10'): ('msp_auroc', 'cifar10-\u003esvhn'),\n",
        "    ('ood_svhn_cropped_msp_auroc', 'cifar100'): ('msp_auroc', 'cifar100-\u003esvhn'),\n",
        "}))\n",
        "\n",
        "corr_matrix = corr_matrix.sort_index(axis=1)\n",
        "corr_matrix = corr_matrix.mean(level=0, axis='columns')\n",
        "corr_matrix = abs(corr_matrix)\n",
        "corr_matrix = corr_matrix.reindex(\n",
        "    corr_matrix.mean().sort_values().index, axis=1)\n",
        "for metric in corr_matrix.columns:\n",
        "  if metric in COMPUTE_METRICS or metric.startswith('score'):\n",
        "    del corr_matrix[metric]\n",
        "corr_matrix = corr_matrix.T.reset_index()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20.0, 5.0))\n",
        "sns.barplot(x='index', y='test_loss', data=corr_matrix)\n",
        "ax.set(xlabel=None)\n",
        "ax.set(ylabel=r'$\\rho(\\cdot,$ test_loss)')\n",
        "\n",
        "filename = 'correlation.png'\n",
        "plt.tight_layout()\n",
        "plt.savefig(filename)\n",
        "colabtools.fileedit.download_file(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2NIeOSKegBz"
      },
      "source": [
        "## Plot Relative Score and Rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt_sCjG-eehS"
      },
      "outputs": [],
      "source": [
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in FEWSHOT_DATASETS]\n",
        "rel_scores, ranks, ranks_by_category = compute_relative_score_and_ranks(df, datasets=datasets)\n",
        "print(\"Average relative score and ranks across categories\")\n",
        "display.display(rel_scores)\n",
        "\n",
        "# Plot rank distribution\n",
        "ax = sns.violinplot(data=ranks.T)\n",
        "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)\n",
        "ax.set_ylabel('Ranking')\n",
        "print(\"==\" * 50)\n",
        "print(\"Rankings\")\n",
        "display.display(ranks)\n",
        "\n",
        "for key in CATEGORIES:\n",
        "  plt.figure()\n",
        "  ax = sns.violinplot(data=ranks_by_category[key].T)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)\n",
        "  ax.set_ylabel('Ranking - %s' % key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MtTqEW-yu7B"
      },
      "source": [
        "# Plotting helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wq6YIte9iIm"
      },
      "outputs": [],
      "source": [
        "#@title Bar plots\n",
        "def plot_in_distribution(df, train_dataset, split):\n",
        "  df = df[df['config.dataset'] == train_dataset].copy()\n",
        "  metrics = [f'{split}_{m}' for m in ['loss', 'prec@1', 'ece', 'calib_auc']]\n",
        "  df = df[['model'] + metrics].melt(\n",
        "      id_vars='model', var_name='metric', value_name='value')\n",
        "  sns.catplot(\n",
        "      col='metric', data=df, y='value', kind='bar', sharey=False, x='model')\n",
        "\n",
        "def plot_ood(df, train_dataset):\n",
        "  df = df[df['config.dataset'] == train_dataset].copy()\n",
        "  if train_dataset == 'imagenet2012':\n",
        "    datasets = {'places365_small'}\n",
        "    metrics = ['msp', 'entropy', 'mlogit']\n",
        "  else:\n",
        "    datasets = set(['svhn_cropped', 'cifar100', 'cifar10']) - {train_dataset}\n",
        "    metrics = ['msp', 'entropy', 'mlogit', 'maha', 'rmaha']\n",
        "  cols = [\n",
        "      f'ood_{ds}_{m}_auroc' for (ds, m) in itertools.product(datasets, metrics)\n",
        "  ]\n",
        "  cols = list(set(cols).intersection(df.columns))\n",
        "  df = df[['model'] + cols]\n",
        "  df = df.melt(id_vars='model', var_name='metric', value_name='AUROC')\n",
        "  df['dataset'] = df['metric'].apply(lambda x: x.split('_')[1])\n",
        "  df['metric'] = df['metric'].apply(lambda x: x.split('_')[-2])\n",
        "\n",
        "  sns.catplot(\n",
        "      data=df, x='metric', y='AUROC', hue='model', kind='bar', col='dataset')\n",
        "  plt.ylim((0.5, 1))\n",
        "\n",
        "\n",
        "def plot_corrupted(df, train_dataset):\n",
        "  df = df[df['config.dataset'] == train_dataset].copy()\n",
        "  ds = 'imagenet_real' if train_dataset == 'imagenet2012' else 'cifar_10h'\n",
        "  metrics = [f'{ds}_{m}' for m in ['loss', 'prec@1', 'ece', 'calib_auc']]\n",
        "  df = df[['model'] + metrics].melt(\n",
        "      id_vars='model', var_name='metric', value_name='value')\n",
        "  sns.catplot(\n",
        "      col='metric', data=df, y='value', kind='bar', sharey=False, x='model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB9ohPCduqdy"
      },
      "outputs": [],
      "source": [
        "#@title Pareto plots\n",
        "\n",
        "def is_on_pareto_front(p, points, higher_is_better):\n",
        "  if higher_is_better:\n",
        "    return len([\n",
        "        point for point in points if point[0] \u003c= p[0] and point[1] \u003e p[1]\n",
        "    ]) == 0\n",
        "  else:\n",
        "    return len([\n",
        "        point for point in points if point[0] \u003c= p[0] and point[1] \u003c p[1]\n",
        "    ]) == 0\n",
        "\n",
        "\n",
        "def get_pareto_points(x, y, higher_is_better):\n",
        "  points = list(zip(x, y))\n",
        "  frontier = [\n",
        "      p for p in points if is_on_pareto_front(p, points, higher_is_better)\n",
        "  ]\n",
        "  return sorted(frontier, key=lambda x: x[0])\n",
        "\n",
        "\n",
        "def plot_fn(data, x, y, **kws):\n",
        "  ax = plt.gca()\n",
        "  sns.scatterplot(data=data, x=x, y=y, hue='model')\n",
        "  for _, point in data.iterrows():\n",
        "    ann = ax.annotate(\n",
        "        '  ' + point['model'],\n",
        "        xy=(point[x], point[y]),\n",
        "        ha='left',\n",
        "        va='bottom',\n",
        "  )\n",
        "\n",
        "  metric = data['metric'].iloc[0]\n",
        "  higher_is_better = 'prec' in metric or 'auc' in metric\n",
        "  pareto_frontier = get_pareto_points(\n",
        "      data[x], data[y], higher_is_better=higher_is_better)\n",
        "  xx, yy = zip(*pareto_frontier)\n",
        "  sns.lineplot(x=xx, y=yy, linestyle='--')\n",
        "\n",
        "def pareto_plot(df, metrics, train_dataset=None,\n",
        "                xmetric='num_params', xlabel='Log # Params'):\n",
        "  df = df[df['config.dataset'] == train_dataset].copy()\n",
        "  df = df.groupby(['model', 'config.dataset', xmetric]\n",
        "                  )[metrics].apply(np.mean).reset_index()\n",
        "  df = df.melt(\n",
        "      id_vars=['model', 'config.dataset', xmetric],\n",
        "      var_name='metric',\n",
        "      value_name='value')\n",
        "\n",
        "  g = sns.FacetGrid(data=df, col='metric', sharey=False, size=5)\n",
        "  g.map_dataframe(plot_fn, x=xmetric, y='value')\n",
        "  g.set_xlabels(xlabel)\n",
        "  g.set(xscale='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBld21j5yx4I"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "186VNwRThIhE"
      },
      "outputs": [],
      "source": [
        "#@title Upstream JFT\n",
        "df = measurements.copy()\n",
        "df = df[df['config.dataset'] == 'jft/entity:1.0.0']\n",
        "df = df[['model', 'val_loss', 'val_prec@1', 'a/imagenet_10shot']].melt(\n",
        "    id_vars='model', var_name='metric', value_name='value')\n",
        "sns.catplot(\n",
        "    col='metric', data=df, x='model', y='value', kind='bar', sharey=False)\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='jft/entity:1.0.0',\n",
        "    metrics=['val_loss', 'val_prec@1', 'a/imagenet_10shot'],\n",
        ")\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='jft/entity:1.0.0',\n",
        "    metrics=['val_loss', 'val_prec@1', 'a/imagenet_10shot'],\n",
        "    xmetric='tpu_days',\n",
        "    xlabel='Compute (TPUv3 core days)',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl7rOkhsuFm0"
      },
      "source": [
        "## Cifar 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzz9lwisoKL3"
      },
      "outputs": [],
      "source": [
        "#@title In-distribution\n",
        "plot_in_distribution(measurements, train_dataset='cifar10', split='test')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='cifar10',\n",
        "    metrics=['test_loss', 'test_prec@1', 'test_ece', 'test_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oote6WoS_QOd"
      },
      "outputs": [],
      "source": [
        "#@title Cifar10h\n",
        "plot_corrupted(measurements, train_dataset='cifar10')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='cifar10',\n",
        "    metrics=['cifar_10h_loss', 'cifar_10h_prec@1', 'cifar_10h_ece', 'cifar_10h_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Afa3nr-8ri"
      },
      "outputs": [],
      "source": [
        "#@title OOD\n",
        "plot_ood(measurements, train_dataset='cifar10')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ2bY0aPlQ5e"
      },
      "source": [
        "## Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q_wtXR-9CU0"
      },
      "outputs": [],
      "source": [
        "#@title In-distribution\n",
        "plot_in_distribution(measurements, train_dataset='cifar100', split='test')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='cifar100',\n",
        "    metrics=['test_loss', 'test_prec@1', 'test_ece', 'test_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j8jKADGx_ZyV"
      },
      "outputs": [],
      "source": [
        "#@title OOD\n",
        "plot_ood(measurements, train_dataset='cifar100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd4Ub0YclSmZ"
      },
      "source": [
        "## Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gokyk90V_nYw"
      },
      "outputs": [],
      "source": [
        "#@title In-distribution\n",
        "plot_in_distribution(measurements, train_dataset='imagenet2012', split='test')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='imagenet2012',\n",
        "    metrics=['test_loss', 'test_prec@1', 'test_ece', 'test_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcx57p3a_r7d"
      },
      "outputs": [],
      "source": [
        "#@title Imagenet Real\n",
        "plot_corrupted(measurements, train_dataset='imagenet2012')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='imagenet2012',\n",
        "    metrics=[\n",
        "        'imagenet_real_loss', 'imagenet_real_prec@1', 'imagenet_real_ece',\n",
        "        'imagenet_real_calib_auc'\n",
        "    ])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "RDL Big Paper Plots",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1Tufx2M784xw4obIzgWXdYAX8lJGtJb3k",
          "timestamp": 1645545840302
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/google/colab/big_paper_experiments.ipynb?workspaceId=trandustin:plots::citc",
          "timestamp": 1645091101660
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/google/colab/big_paper_experiments.ipynb?cl=428611591",
          "timestamp": 1644888762710
        },
        {
          "file_id": "1pql3UgJFiEjGW4igFnWING7A73O_iq04",
          "timestamp": 1644878348078
        },
        {
          "file_id": "1_OgnYgLLR0zpaN2-bBQt1RE3B5JktWNN",
          "timestamp": 1643738065376
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
