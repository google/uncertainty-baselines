{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZR1tYwA2o8C"
      },
      "source": [
        "# RDL Big Paper Plots\n",
        "\n",
        "*Licensed under the Apache License, Version 2.0.*\n",
        "\n",
        "To run this in a public Colab, change the GitHub link: replace github.com with [githubtocolab.com](http://githubtocolab.com).\n",
        "\n",
        "This colab loads raw measurements from disk and analyzes the results.\n",
        "\n",
        "## Choosing optimal hyperparameters\n",
        "We automatically detect hyperparameter sweeps by selecting fields that don't correspond to dataset metrics but that have more than one chosen value. We choose the hyperparameters that achieve the best according a given metric (see `dataset_metric`) after averaging over random seeds. For example, if the model is trained on CIFAR-10, we use CIFAR-10's validation loss.\n",
        "\n",
        "## Plots\n",
        "All plots report the performance of a given model according to its optimal hyperparameters chosen above. When there are runs with multiple seeds, we show the mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3lFstNIFvry"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "import itertools\n",
        "\n",
        "import colabtools.fileedit\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "colab_utils = None\n",
        "\n",
        "if colab_utils is None:\n",
        "  !rm -rf uncertainty-baselines\n",
        "  !git clone https://github.com/google/uncertainty-baselines.git\n",
        "  !cp uncertainty-baselines/experimental/big_paper/colab_utils.py .\n",
        "  import colab_utils\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.reset_orig()\n",
        "sns.set_theme()\n",
        "matplotlib.rcParams['figure.dpi'] = 1000\n",
        "matplotlib.rcParams['lines.linewidth'] = 1.25\n",
        "sns.set_style(\"white\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qpT4rtuEN14"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4JuGWk_t6FW"
      },
      "outputs": [],
      "source": [
        "#@title Choosing optimal hyperparameters\n",
        "\n",
        "# The finetuning deterministic jobs use a fixed random seed but different\n",
        "# upstream checkpoints, which themselves correspond to different random seeds.\n",
        "# In this case, we thus marginalize over upstream checkpoints\n",
        "# (`config.model_init`) rather than the random seed.\n",
        "\n",
        "DATASET_METRIC = {\n",
        "    'cifar10': 'val_loss',\n",
        "    'cifar100': 'val_loss',\n",
        "    'imagenet2012': 'val_loss',\n",
        "    'imagenet21k': 'val_loss',\n",
        "    'jft/entity:1.0.0': 'val_loss',\n",
        "    'retina_country': 'in_domain_validation/auroc',\n",
        "    'retina_severity': 'in_domain_validation/auroc',\n",
        "    'imagenet_variants': 'imagenet/nll',\n",
        "}\n",
        "\n",
        "\n",
        "def get_optimal_results(measurements: Dict[str, pd.DataFrame],\n",
        "                        dataset_metric: Dict[str, str] = DATASET_METRIC,\n",
        "                        verbose=True) -\u003e pd.DataFrame:\n",
        "  \"\"\"Returns a dataframe, typically with one result per model type.\n",
        "\n",
        "  A model type may have multiple results that will be averaged over when\n",
        "  plotting (e.g., random seeds).\n",
        "\n",
        "  Args:\n",
        "    measurements: Dictionary of dataframes to obtain best results for.\n",
        "    dataset_metric: Each dataset's metric to tune for, in the format\n",
        "      `{dataset: metric}`.\n",
        "  \"\"\"\n",
        "  results = []\n",
        "  model_tuple = ('Det', 'Det I21K', 'DE', 'DE S/32', 'DE B/32', 'DE L/32')\n",
        "  for k, v in measurements.items():\n",
        "    marginalization_hparams = (colab_utils.random_seed_col(),)\n",
        "    if k in model_tuple:\n",
        "      marginalization_hparams += ('config.model_init',)\n",
        "    for ds in v[colab_utils.dataset_col()].unique():\n",
        "      df = v[v[colab_utils.dataset_col()] == ds]\n",
        "      try:\n",
        "        results.append(\n",
        "            colab_utils.get_tuned_results(\n",
        "                df,\n",
        "                tuning_metric=dataset_metric[ds],\n",
        "                marginalization_hparams=marginalization_hparams,\n",
        "                verbose=verbose))\n",
        "      except KeyError:\n",
        "        print(f'Could not get optimal results for {k}, {ds}.')\n",
        "    print()\n",
        "  return pd.concat(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mMdPlsI12gec"
      },
      "outputs": [],
      "source": [
        "#@title Pretty printing \n",
        "\n",
        "def pprint(df, models=None, exclude_models=None):\n",
        "  \"\"\"Pretty print dataframe.\n",
        "\n",
        "  Args:\n",
        "    df: Dataframe.\n",
        "    models: Optional list of models to only show. Useful for comparing specific\n",
        "      models to see which performs better (highlighted cells).\n",
        "    exclude_models: Optional list of models to exclude.\n",
        "  \"\"\"\n",
        "  def _rename(m):\n",
        "    m = m.replace('cifar_10h', 'cifar10h')\n",
        "    m = m.replace('places365_small', 'places365')\n",
        "    m = m.replace('imagenet_', 'imagenet-')\n",
        "    m = m.replace('/mean', '')\n",
        "    m = m.replace('/', ' ')\n",
        "    m = m.replace('_', ' ')\n",
        "    m = m.replace('cropped ', '')\n",
        "    m = m.replace('ood', '')\n",
        "    m = m.replace('ece', 'ECE')\n",
        "    m = m.replace('auc', 'AUC')\n",
        "    m = m.replace('auroc', 'AUROC')\n",
        "    m = m.replace('loss', 'NLL')\n",
        "    m = m.replace('negative log likelih', 'NLL')\n",
        "    m = m.replace('nll', 'NLL')\n",
        "    m = m.replace('brier', 'Brier')\n",
        "    m = m.replace('mce', 'mCE')\n",
        "    m = m.replace('pmk', 'p-mk')\n",
        "    return m\n",
        "  def _formatter(metric):\n",
        "    if any(x in metric for x in ['AUROC', 'AUC']):\n",
        "      return '{:.2f}'.format\n",
        "    elif any(x in metric for x in ['prec', 'ECE', 'accuracy']):\n",
        "      return lambda x: '{:.1f}%'.format(x * 100)\n",
        "    elif any(x in metric for x in ['score', 'exaflops', 'tpu days', 'gflops', \n",
        "                                   'ms step']):\n",
        "      return lambda x: '{:.1f}'.format(x)\n",
        "    elif any(x in metric for x in ['NLL', 'Brier']):\n",
        "      return '{:.3f}'.format\n",
        "    else:\n",
        "      return lambda x: x\n",
        "  def _highlight(data, color='#90EE90'):\n",
        "    attr = 'background-color: {}'.format(color)\n",
        "    data = data.replace('%','', regex=True).astype(float)\n",
        "    if any(x in data.name[1] for x in ['NLL', 'ECE', 'Brier', 'mCE',\n",
        "                                       'relative mCE', 'accuracy drop',\n",
        "                                       'accuracy pm-k']):\n",
        "      is_best = data == data.min()\n",
        "    elif any(x in data.name[1] for x in ['exaflops', 'tpu days', 'gflops',\n",
        "                                         'ms step']):\n",
        "      is_best = data == 'asdf'\n",
        "    else:\n",
        "      is_best = data == data.max()\n",
        "    return [attr if v else '' for v in is_best]\n",
        "\n",
        "  df = df.copy()\n",
        "  df = df.rename(columns=_rename)\n",
        "  for c in df:\n",
        "    df[c] = df[c].apply(_formatter(c[0]))\n",
        "\n",
        "  # Swap order of column's multiindex to be dataset first.\n",
        "  df.columns = df.columns.swaplevel(0, 1)\n",
        "  df = df.sort_index(axis=1, level=0)\n",
        "\n",
        "  df = df.T\n",
        "  if models is not None:\n",
        "    df = df[[c for c in df.columns if c in models]]\n",
        "  elif exclude_models is not None:\n",
        "    df = df[[c for c in df.columns if c not in exclude_models]]\n",
        "\n",
        "  return display.display(df.style.apply(_highlight, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TBqSMsgf0U2k",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#@title RETINA\n",
        "REBUILD_RETINA_RESULTS_CACHE = False\n",
        "\n",
        "if REBUILD_RETINA_RESULTS_CACHE:\n",
        "  import os\n",
        "  os.system('pip install wandb')\n",
        "  import wandb\n",
        "\n",
        "# TODO(nband): add grid search results (currently random search).\n",
        "RETINA_SHIFT_AND_UQ_METHOD_TO_WANDB = {\n",
        "  ('aptos', 'deterministic'): 'vit32-finetune-aptos-deterministic-focused-3',\n",
        "  ('aptos', 'batchensemble'): 'vit32-finetune-aptos-batchensemble',\n",
        "  ('severity', 'deterministic'): 'vit32-finetune-severity-deterministic',\n",
        "  ('severity', 'batchensemble'): 'vit32-finetune-severity-batchensemble-focused-1'\n",
        "}\n",
        "\n",
        "RETINA_SHIFTS = ['aptos', 'severity']\n",
        "RETINA_UQ_METHODS = ['deterministic', 'batchensemble']\n",
        "RETINA_UQ_METHOD_TO_DF_NAME = {\n",
        "    'deterministic': 'Det I21K',\n",
        "    'batchensemble': 'BE L/32 (I21K)'\n",
        "}\n",
        "\n",
        "RETINA_SHIFT_TO_METRICS = {\n",
        "  'aptos': [\n",
        "    # In-Domain\n",
        "    'in_domain_test.in_domain_test/accuracy',\n",
        "    'in_domain_test.in_domain_test/negative_log_likelihood',\n",
        "    'in_domain_test.in_domain_test/ece',\n",
        "    'in_domain_test.in_domain_test/retention_auroc_auc',\n",
        "    # OOD\n",
        "    'ood_test.ood_test/accuracy',\n",
        "    'ood_test.ood_test/negative_log_likelihood',\n",
        "    'ood_test.ood_test/ece',\n",
        "    'ood_test.ood_test/retention_auroc_auc'\n",
        "  ],\n",
        "  'severity': [\n",
        "    # In-Domain\n",
        "    'in_domain_test.in_domain_test/accuracy',\n",
        "    'in_domain_test.in_domain_test/negative_log_likelihood',\n",
        "    'in_domain_test.in_domain_test/ece',\n",
        "    'in_domain_test.in_domain_test/retention_auroc_auc',\n",
        "    # OOD\n",
        "    'ood_test.ood_test/accuracy',\n",
        "    'ood_test.ood_test/negative_log_likelihood',\n",
        "    'ood_test.ood_test/ece',\n",
        "    'ood_test.ood_test/retention_accuracy_auc'\n",
        "  ]\n",
        "}\n",
        "RETINA_MODEL_SELECTION_METRIC = 'in_domain_validation.in_domain_validation/auroc'\n",
        "\n",
        "# Split RETINA results into the two distributional shifts: Country Shift and\n",
        "# Severity Shift.\n",
        "\n",
        "SHIFT_MAP = {'aptos': 'country', 'severity': 'severity'}\n",
        "\n",
        "\n",
        "def select_top_model_from_project(project_name):\n",
        "  api = wandb.Api(timeout=100000000)\n",
        "  runs = api.runs(project_name)\n",
        "  print(f'Retrieved run results from Weights \u0026 Biases project {project_name}.')\n",
        "  sweep_history_df = []\n",
        "\n",
        "  # Get all full histories\n",
        "  for run in runs:\n",
        "    run_history_df = pd.DataFrame(run._full_history())\n",
        "\n",
        "    # Add run name\n",
        "    run_history_df['run_name'] = run.name\n",
        "    sweep_history_df.append(run_history_df)\n",
        "\n",
        "  sweep_history_df = pd.concat(sweep_history_df)\n",
        "  sweep_history_df.reset_index(inplace=True)\n",
        "\n",
        "  # Best performing step of the best performing model\n",
        "  top_idx = sweep_history_df[RETINA_MODEL_SELECTION_METRIC].idxmax()\n",
        "  return sweep_history_df.iloc[top_idx]\n",
        "\n",
        "\n",
        "def get_retina_i21k_results_df():\n",
        "  all_results_df = []\n",
        "  for shift in RETINA_SHIFTS:\n",
        "    for uq_method in RETINA_UQ_METHODS:\n",
        "      print(f'Retrieving results from shift {shift}, '\n",
        "            f'uncertainty quantification method {uq_method}.')\n",
        "      wandb_project = RETINA_SHIFT_AND_UQ_METHOD_TO_WANDB[(shift, uq_method)]\n",
        "      model_results = select_top_model_from_project(wandb_project)\n",
        "      result_df = model_results.to_frame().T\n",
        "      result_df['shift'] = shift\n",
        "      result_df['uq_method'] = uq_method\n",
        "      all_results_df.append(result_df)\n",
        "\n",
        "  return pd.concat(all_results_df)\n",
        "\n",
        "\n",
        "def add_retina_i21k_results(retina_results_df, preprocessed_df, shift_map=SHIFT_MAP):\n",
        "  for shift in RETINA_SHIFTS:\n",
        "    for uq_method in RETINA_UQ_METHODS:\n",
        "      print(f'Adding results from shift {shift}, '\n",
        "            f'uncertainty quantification method {uq_method}.')\n",
        "      model_results = retina_results_df[\n",
        "        (retina_results_df['shift'] == shift) \u0026\n",
        "        (retina_results_df['uq_method'] == uq_method)]\n",
        "      n_results = len(model_results)\n",
        "      assert n_results == 1, f'Found {n_results} model results, expected 1.'\n",
        "      model_results = model_results.iloc[0]\n",
        "      metrics = RETINA_SHIFT_TO_METRICS[shift]\n",
        "      for metric in metrics:\n",
        "        df_metric_name = metric.split('.')[1]\n",
        "        per_metric_result = model_results[metric]\n",
        "        shift_df_name = shift_map[shift]\n",
        "        metric_shift_series = preprocessed_df[(\n",
        "          df_metric_name, f'retina_{shift_df_name}')]\n",
        "        metric_shift_series[\n",
        "          RETINA_UQ_METHOD_TO_DF_NAME[uq_method]] = per_metric_result\n",
        "        preprocessed_df[\n",
        "          (df_metric_name, f'retina_{shift_df_name}')] = metric_shift_series\n",
        "\n",
        "  return preprocessed_df\n",
        "\n",
        "if REBUILD_RETINA_RESULTS_CACHE:\n",
        "  # Retrieve RETINA I21K results from Weights \u0026 Biases\n",
        "  retina_i21k_results_df = get_retina_i21k_results_df()\n",
        "\n",
        "  # Store RETINA results in gs bucket\n",
        "  retina_ub_gs_file_path = 'gs://retina-i21k-results-df/retina-i21k-results.tsv'\n",
        "  with tf.io.gfile.GFile(retina_ub_gs_file_path, 'w') as f:\n",
        "    retina_i21k_results_df.to_csv(f, sep='\\t', index=None)\n",
        "\n",
        "\n",
        "def add_distribution_shift_to_retina_ds_name(row):\n",
        "  dataset = str(row['config.dataset'])\n",
        "  if dataset == 'retina':\n",
        "    shift = SHIFT_MAP[str(row['config.distribution_shift'])]\n",
        "    row['config.dataset'] = f'{dataset}_{shift}'\n",
        "\n",
        "  return row\n",
        "\n",
        "def split_retina_results_by_shifts(raw_dict):\n",
        "  for model in raw_dict.keys():\n",
        "    raw_model_df = raw_dict[model]\n",
        "    if not len(raw_model_df[raw_model_df['config.dataset'] == 'retina']):\n",
        "        continue\n",
        "\n",
        "    print(f'Splitting RETINA results for model {model} by distribution shift.')\n",
        "\n",
        "    raw_model_df = raw_model_df.apply(\n",
        "        add_distribution_shift_to_retina_ds_name, axis='columns')\n",
        "    raw_dict[model] = raw_model_df\n",
        "\n",
        "  return raw_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GygKbZFjwiLV"
      },
      "source": [
        "## Load and preprocess measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFG3GNLoQN3a"
      },
      "outputs": [],
      "source": [
        "load_from_cloud = True\n",
        "if load_from_cloud == True:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  project_id = 'marginalization-external-xgcp'\n",
        "  !gcloud config set project {project_id}\n",
        "\n",
        "  measurements_path = '/tmp/big-paper-raw-measurements.pkl'\n",
        "  !gsutil cp gs://ub-checkpoints/big-paper-raw-measurements.pkl {measurements_path}\n",
        "\n",
        "  retina_path = '/tmp/retina-i21k-results.tsv'\n",
        "  !gsutil cp gs://retina-i21k-results-df/retina-i21k-results.tsv {retina_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcnAzeyWsHu8"
      },
      "outputs": [],
      "source": [
        "with tf.io.gfile.GFile(measurements_path, 'rb') as f:\n",
        "  raw_measurements = pickle.load(f)\n",
        "\n",
        "with tf.io.gfile.GFile(retina_path, 'r') as f:\n",
        "  retina_i21k_results_df = pd.read_csv(f, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Bc5TFN3ZTk"
      },
      "outputs": [],
      "source": [
        "raw_measurements = split_retina_results_by_shifts(raw_measurements)\n",
        "\n",
        "excluded_keys = [\n",
        "    'DE', 'Det-\u003eDE', 'DE S/32', 'Det-\u003eDE S/32', 'DE B/32', 'Det-\u003eDE B/32',\n",
        "    'DE L/32', 'Det-\u003eDE L/32', 'Det -\u003e BE L/32 (n=2)', 'Det -\u003e BE L/32 (n=4)',\n",
        "    'Det -\u003e BE L/32 (n=8)'\n",
        "]\n",
        "included_measurements = {\n",
        "    k: v for k, v in raw_measurements.items() if k not in excluded_keys\n",
        "}\n",
        "included_measurements['DE'] = raw_measurements['DE L/32'].query(\n",
        "    'ensemble_size == 3')\n",
        "included_measurements['Det-\u003eDE'] = raw_measurements['Det-\u003eDE L/32'].query(\n",
        "    'ensemble_size == 3')\n",
        "measurements = get_optimal_results(included_measurements)\n",
        "\n",
        "df = colab_utils.process_tuned_results(measurements)\n",
        "df = add_retina_i21k_results(\n",
        "    retina_results_df=retina_i21k_results_df, preprocessed_df=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuV4-5wzEQiE"
      },
      "source": [
        "## Compute reliability score and generate table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kRH3CQGwaQg"
      },
      "outputs": [],
      "source": [
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "    # 'imagenet_variants',\n",
        "    # 'retina_country',\n",
        "    # 'retina_severity',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in colab_utils.default_fewshot_datasets()]\n",
        "\n",
        "scores = colab_utils.compute_score(\n",
        "    df, datasets=datasets, drop_1shot=True,\n",
        "    drop_incomplete_measurements=True) * 100\n",
        "\n",
        "score_cols = [\n",
        "    'score', 'score_prediction', 'score_uncertainty', 'score_adaptation'\n",
        "]\n",
        "display.display(scores[score_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQS1I1pRD9rm"
      },
      "outputs": [],
      "source": [
        "df_with_scores = df.copy()\n",
        "for column in score_cols:\n",
        "  df_with_scores[column] = scores[column]\n",
        "\n",
        "pprint(\n",
        "    df_with_scores,\n",
        "    # models=['BE L/32', 'Det'],\n",
        "    # exclude_models=['DE', 'Det-\u003eDE'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RySd-77qMBe"
      },
      "outputs": [],
      "source": [
        "# Show a subset of the table's metrics + models\n",
        "metrics = ['score', 'score_prediction', 'score_uncertainty', 'score_adaptation',\n",
        "           'exaflops', 'test_loss', 'tpu_days']\n",
        "models = ['BE L/32', 'Det', 'GP', 'Het', 'BE L/32 (I21K)', 'Det I21K',\n",
        "          'BE-\u003eBE+Het']\n",
        "pprint(df_with_scores.loc[models][metrics].rename(\n",
        "    columns={'compute': 'z/compute'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBtgUevvEYrh"
      },
      "source": [
        "## Plot reliability score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5qvJhS_YWwE"
      },
      "outputs": [],
      "source": [
        "def pareto_plot(df, x, y, ax, filename=None, **kwargs):\n",
        "  def is_on_pareto_front(p, points, higher_is_better):\n",
        "    if higher_is_better:\n",
        "      return len([\n",
        "          point for point in points if point[0] \u003c= p[0] and point[1] \u003e p[1]\n",
        "      ]) == 0\n",
        "    else:\n",
        "      return len([\n",
        "          point for point in points if point[0] \u003c= p[0] and point[1] \u003c p[1]\n",
        "      ]) == 0\n",
        "  def get_pareto_points(x, y, higher_is_better=True):\n",
        "    points = list(zip(x, y))\n",
        "    frontier = [\n",
        "        p for p in points if is_on_pareto_front(p, points, higher_is_better)\n",
        "    ]\n",
        "    return sorted(frontier, key=lambda x: x[0])\n",
        "  for model, point in df.iterrows():\n",
        "    ann = ax.annotate(\n",
        "        '  ' + model,\n",
        "        xy=(point[x], point[y]),\n",
        "        ha='left',\n",
        "        va='bottom',\n",
        "  )\n",
        "  sns.scatterplot(x=df[x], y=df[y], ax=ax)\n",
        "  pareto_frontier = get_pareto_points(df[x], df[y])\n",
        "  xx, yy = zip(*pareto_frontier)\n",
        "  sns.lineplot(x=xx, y=yy, linestyle='--', ax=ax)\n",
        "  ax.set(xscale='log', **kwargs)\n",
        "  if filename is not None:\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    colabtools.fileedit.download_file(filename)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10.0, 5.0))\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=ax,\n",
        "    y='score',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel='Compute (TPUv3 core days)',\n",
        "    ylabel='Reliability Score',\n",
        "    filename='reliability.png',\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(3.5 * 3, 3.5))\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=axes[0],\n",
        "    y='score_prediction',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel=None,\n",
        "    ylabel=None,\n",
        "    title='Reliability Score (Prediction)',\n",
        ")\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=axes[1],\n",
        "    y='score_uncertainty',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel=None,\n",
        "    ylabel=None,\n",
        "    title='Reliability Score (Uncertainty)',\n",
        ")\n",
        "pareto_plot(\n",
        "    df_with_scores[[x.startswith('BE') for x in df_with_scores.index.values]],\n",
        "    ax=axes[2],\n",
        "    y='score_adaptation',\n",
        "    x=('tpu_days', 'compute'),\n",
        "    xlabel=None,\n",
        "    ylabel=None,\n",
        "    title='Reliability Score (Adaptation)',\n",
        ")\n",
        "filename = 'reliability_components.png'\n",
        "plt.tight_layout()\n",
        "plt.savefig(filename)\n",
        "colabtools.fileedit.download_file(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj2HLvvlEg46"
      },
      "source": [
        "## Analyze correlation of metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIaSlPrx8JLB"
      },
      "outputs": [],
      "source": [
        "temp_df = colab_utils.process_tuned_results(\n",
        "    measurements,\n",
        "    relevant_metrics=colab_utils.default_selected_metrics() +\n",
        "    ['training_loss', 'training_prec@1'])\n",
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in colab_utils.default_fewshot_datasets()]\n",
        "temp_scores = colab_utils.compute_score(\n",
        "    temp_df,\n",
        "    datasets=datasets,\n",
        "    drop_1shot=True,\n",
        "    drop_incomplete_measurements=True)\n",
        "for column in score_cols:\n",
        "  temp_df[column] = temp_scores[column]\n",
        "\n",
        "# scores correlation matrix\n",
        "columns = ['score', 'score_prediction', 'score_uncertainty', 'score_adaptation']\n",
        "corr_matrix = temp_df[columns]\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "corr_matrix = corr_matrix.corr()\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# upstream test metrics\n",
        "metrics = ['score', 'score_prediction', 'score_uncertainty', 'score_adaptation']\n",
        "corr_matrix = temp_df.corr()[['test_loss', 'test_prec@1']].T.xs(\n",
        "    'jft/entity:1.0.0', level='dataset')\n",
        "corr_matrix = corr_matrix[metrics]\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# imagenet 10-shot. It doesn't correlate well with reliability, mostly due to\n",
        "# it not correlating well surprisingly on other few-shot tasks.\n",
        "corr_matrix = temp_df.corr()[['10shot_prec@1']].T.xs(\n",
        "    'few-shot imagenet', level='dataset')\n",
        "corr_matrix = corr_matrix[metrics]\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# downstream training loss. The correlation is not nearly as tight as on\n",
        "# upstream.\n",
        "corr_matrix = temp_df.corr()[['training_loss']].T\n",
        "corr_matrix = corr_matrix[metrics + ['test_loss']]\n",
        "corr_matrix = corr_matrix.drop(index=('training_loss', 'retina_country'))\n",
        "corr_matrix = corr_matrix.drop(index=('training_loss', 'retina_severity'))\n",
        "corr_matrix = corr_matrix.drop(index=('training_loss', 'imagenet21k'))\n",
        "corr_matrix = corr_matrix.drop(columns=('test_loss', 'imagenet21k'))\n",
        "# Display test loss only for training loss' same downstream dataset. Looking at\n",
        "# cifar10's train loss correlation with I1K's test loss isn't meaningful.\n",
        "test_loss = pd.Series(\n",
        "    np.diag(corr_matrix['test_loss']), index=corr_matrix['test_loss'].index)\n",
        "corr_matrix = corr_matrix.drop(columns='test_loss')\n",
        "corr_matrix['test_loss'] = test_loss\n",
        "corr_matrix.columns = [''.join(col) for col in corr_matrix.columns.values]\n",
        "display.display(corr_matrix)\n",
        "\n",
        "# Similar to old plot in go/rdl-big-meeting, even generalization gap decreases.\n",
        "# And downstream is not very indicative, but upstream is.\n",
        "temp_df2 = temp_df.copy()\n",
        "for d in temp_df2['test_loss'].columns:\n",
        "  temp_df2['reg_loss',\n",
        "           d] = temp_df2['test_loss', d] - temp_df2['training_loss', d]\n",
        "\n",
        "corr_matrix = temp_df2.corr()[['reg_loss']].T\n",
        "corr_matrix = corr_matrix[metrics + ['training_loss']]\n",
        "corr_matrix = corr_matrix.drop(index=('reg_loss', 'imagenet21k'))\n",
        "display.display(corr_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_ShrtSoVQwh"
      },
      "outputs": [],
      "source": [
        "corr_matrix = temp_df.corr()[['test_loss', 'test_prec@1', 'training_loss']].T.xs('jft/entity:1.0.0', level='dataset')\n",
        "\n",
        "# Rename certain task metrics to be under their generic metric name. This way,\n",
        "# we can average values across that metric.\n",
        "corr_matrix.columns = corr_matrix.columns.values\n",
        "corr_matrix.columns = pd.MultiIndex.from_tuples(corr_matrix.rename(columns={\n",
        "    ('imagenet_real_calib_auc', 'imagenet2012'): ('test_calib_auc', 'imagenet_real'),\n",
        "    ('imagenet_real_ece', 'imagenet2012'): ('test_ece', 'imagenet_real'),\n",
        "    ('imagenet_real_loss', 'imagenet2012'): ('test_loss', 'imagenet_real'),\n",
        "    ('imagenet_real_prec@1', 'imagenet2012'): ('test_prec@1', 'imagenet_real'),\n",
        "    ('cifar_10h_calib_auc', 'cifar10'): ('test_calib_auc', 'cifar_10h'),\n",
        "    ('cifar_10h_ece', 'cifar10'): ('test_ece', 'cifar_10h'),\n",
        "    ('cifar_10h_loss', 'cifar10'): ('test_loss', 'cifar_10h'),\n",
        "    ('cifar_10h_prec@1', 'cifar10'): ('test_prec@1', 'cifar_10h'),\n",
        "    ('ood_cifar100_msp_auroc', 'cifar10'): ('msp_auroc', 'cifar10-\u003ecifar100'),\n",
        "    ('ood_cifar10_msp_auroc', 'cifar100'): ('msp_auroc', 'cifar100-\u003ecifar10'),\n",
        "    ('ood_places365_small_msp_auroc', 'imagenet2012'): ('msp_auroc', 'imagenet2012-\u003eplaces365'),\n",
        "    ('ood_svhn_cropped_msp_auroc', 'cifar10'): ('msp_auroc', 'cifar10-\u003esvhn'),\n",
        "    ('ood_svhn_cropped_msp_auroc', 'cifar100'): ('msp_auroc', 'cifar100-\u003esvhn'),\n",
        "}))\n",
        "\n",
        "corr_matrix = corr_matrix.sort_index(axis=1)\n",
        "corr_matrix = corr_matrix.mean(level=0, axis='columns')\n",
        "corr_matrix = abs(corr_matrix)\n",
        "corr_matrix = corr_matrix.reindex(\n",
        "    corr_matrix.mean().sort_values().index, axis=1)\n",
        "for metric in corr_matrix.columns:\n",
        "  if metric.startswith('score') or metric in ['exaflops', 'tpu_days', 'gflops', 'ms_step']:\n",
        "    del corr_matrix[metric]\n",
        "corr_matrix = corr_matrix.T.reset_index()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20.0, 5.0))\n",
        "sns.barplot(x='index', y='test_loss', data=corr_matrix)\n",
        "ax.set(xlabel=None)\n",
        "ax.set(ylabel=r'$\\rho(\\cdot,$ test_loss)')\n",
        "\n",
        "filename = 'correlation.png'\n",
        "plt.tight_layout()\n",
        "plt.savefig(filename)\n",
        "colabtools.fileedit.download_file(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2NIeOSKegBz"
      },
      "source": [
        "## Plot Relative Score and Rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUGL0zAxOd73"
      },
      "outputs": [],
      "source": [
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "    # 'imagenet_variants',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in colab_utils.default_fewshot_datasets()]\n",
        "rel_scores = colab_utils.compute_score(\n",
        "    df,\n",
        "    drop_1shot=True,\n",
        "    datasets=datasets,\n",
        "    baseline_model='Det',\n",
        "    drop_incomplete_measurements=True)\n",
        "\n",
        "print(\"Average relative score and ranks across categories\")\n",
        "display.display(rel_scores)\n",
        "\n",
        "# Plot rank distribution\n",
        "ranks = colab_utils.rank_models(\n",
        "    df, drop_1shot=True, datasets=datasets, drop_incomplete_measurements=True)\n",
        "ax = sns.violinplot(data=ranks.T)\n",
        "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)\n",
        "ax.set_ylabel('Ranking')\n",
        "print(\"==\" * 50)\n",
        "print(\"Rankings\")\n",
        "display.display(ranks)\n",
        "\n",
        "ranks_by_category = colab_utils.rank_models_by_category(\n",
        "    df, drop_1shot=True, datasets=datasets, drop_incomplete_measurements=False)\n",
        "for key, rank_df in ranks_by_category.items():\n",
        "  plt.figure()\n",
        "  ax = sns.violinplot(data=rank_df.T)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)\n",
        "  ax.set_ylabel('Ranking - %s' % key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MtTqEW-yu7B"
      },
      "source": [
        "# Plotting helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wq6YIte9iIm"
      },
      "outputs": [],
      "source": [
        "#@title Bar plots\n",
        "def plot_metrics(df, train_dataset, metrics):\n",
        "  df = df[df['config.dataset'] == train_dataset].copy()\n",
        "  df = df[['model'] + metrics].melt(\n",
        "      id_vars='model', var_name='metric', value_name='value')\n",
        "  grid = sns.catplot(\n",
        "      col='metric', data=df, y='value', kind='bar', sharey=False,\n",
        "      x='model')\n",
        "  for ax in grid.axes.flat:\n",
        "    ax.set_xticklabels(\n",
        "        ax.get_xticklabels(), rotation=40, horizontalalignment=\"right\"\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_in_distribution(df, train_dataset, split):\n",
        "  metrics = [f'{split}_{m}' for m in ['loss', 'prec@1', 'ece', 'calib_auc']]\n",
        "  plot_metrics(df, train_dataset, metrics)\n",
        "\n",
        "\n",
        "def plot_ood(df, train_dataset):\n",
        "  df = df[df['config.dataset'] == train_dataset].copy()\n",
        "  if train_dataset == 'imagenet2012':\n",
        "    datasets = {'places365_small'}\n",
        "    metrics = ['msp', 'entropy', 'mlogit']\n",
        "  else:\n",
        "    datasets = set(['svhn_cropped', 'cifar100', 'cifar10']) - {train_dataset}\n",
        "    metrics = ['msp', 'entropy', 'mlogit', 'maha', 'rmaha']\n",
        "  cols = [\n",
        "      f'ood_{ds}_{m}_auroc' for (ds, m) in itertools.product(datasets, metrics)\n",
        "  ]\n",
        "  cols = list(set(cols).intersection(df.columns))\n",
        "  df = df[['model'] + cols]\n",
        "  df = df.melt(id_vars='model', var_name='metric', value_name='AUROC')\n",
        "  df['dataset'] = df['metric'].apply(lambda x: x.split('_')[1])\n",
        "  df['metric'] = df['metric'].apply(lambda x: x.split('_')[-2])\n",
        "\n",
        "  sns.catplot(\n",
        "      data=df, x='metric', y='AUROC', hue='model', kind='bar', col='dataset')\n",
        "  plt.ylim((0.5, 1))\n",
        "\n",
        "\n",
        "def plot_reclassified(df, train_dataset):\n",
        "  ds = 'imagenet_real' if train_dataset == 'imagenet2012' else 'cifar_10h'\n",
        "  metrics = [f'{ds}_{m}' for m in ['loss', 'prec@1', 'ece', 'calib_auc']]\n",
        "  plot_metrics(df, train_dataset, metrics)\n",
        "\n",
        "\n",
        "def _get_imagenet_shifts_metrics(eval_dataset):\n",
        "  base_metrics = ['accuracy', 'ece', 'nll', 'brier']\n",
        "  metrics = [f'{eval_dataset}/{m}' for m in base_metrics]\n",
        "  if eval_dataset == 'imagenet_c':\n",
        "    metrics = [f'{m}/mean' for m in metrics]\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def _get_imagenet_robustness_metrics(eval_dataset):\n",
        "  base_metrics = ['accuracy_pmk', 'anchor_accuracy', 'accuracy_drop']\n",
        "  return [f'{eval_dataset}/{m}' for m in base_metrics]\n",
        "\n",
        "\n",
        "def plot_imagenet_shifts(df, eval_dataset):\n",
        "  metrics = _get_imagenet_shifts_metrics(eval_dataset)\n",
        "  plot_metrics(df, 'imagenet_variants', metrics)\n",
        "\n",
        "\n",
        "def plot_imagenet_robustness(df, eval_dataset):\n",
        "  metrics = _get_imagenet_robustness_metrics(eval_dataset)\n",
        "  plot_metrics(df, 'imagenet_variants', metrics)\n",
        "\n",
        "\n",
        "def pareto_plot_imagenet_shifts(df, eval_dataset):\n",
        "  metrics = _get_imagenet_shifts_metrics(eval_dataset)\n",
        "  pareto_plot(df, train_dataset='imagenet_variants', metrics=metrics)\n",
        "\n",
        "\n",
        "def pareto_plot_imagenet_robustness(df, eval_dataset):\n",
        "  metrics = _get_imagenet_robustness_metrics(eval_dataset)\n",
        "  pareto_plot(df, train_dataset='imagenet_variants', metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB9ohPCduqdy"
      },
      "outputs": [],
      "source": [
        "#@title Pareto plots\n",
        "\n",
        "def is_on_pareto_front(p, points, higher_is_better):\n",
        "  if higher_is_better:\n",
        "    return len([\n",
        "        point for point in points if point[0] \u003c= p[0] and point[1] \u003e p[1]\n",
        "    ]) == 0\n",
        "  else:\n",
        "    return len([\n",
        "        point for point in points if point[0] \u003c= p[0] and point[1] \u003c p[1]\n",
        "    ]) == 0\n",
        "\n",
        "\n",
        "def get_pareto_points(x, y, higher_is_better):\n",
        "  points = list(zip(x, y))\n",
        "  frontier = [\n",
        "      p for p in points if is_on_pareto_front(p, points, higher_is_better)\n",
        "  ]\n",
        "  return sorted(frontier, key=lambda x: x[0])\n",
        "\n",
        "\n",
        "def plot_fn(data, x, y, **kws):\n",
        "  ax = plt.gca()\n",
        "  sns.scatterplot(data=data, x=x, y=y, hue='model')\n",
        "  for _, point in data.iterrows():\n",
        "    ann = ax.annotate(\n",
        "        '  ' + point['model'],\n",
        "        xy=(point[x], point[y]),\n",
        "        ha='left',\n",
        "        va='bottom',\n",
        "  )\n",
        "\n",
        "  metric = data['metric'].iloc[0]\n",
        "  higher_is_better = colab_utils.is_higher_better(metric)\n",
        "  pareto_frontier = get_pareto_points(\n",
        "      data[x], data[y], higher_is_better=higher_is_better)\n",
        "  xx, yy = zip(*pareto_frontier)\n",
        "  sns.lineplot(x=xx, y=yy, linestyle='--')\n",
        "\n",
        "\n",
        "def pareto_plot(df, metrics, train_dataset=None,\n",
        "                xmetric='num_params', xlabel='Log # Params'):\n",
        "  df = df[df['config.dataset'] == train_dataset].copy()\n",
        "  df = df.groupby(['model', 'config.dataset', xmetric]\n",
        "                  )[metrics].apply(np.mean).reset_index()\n",
        "  df = df.melt(\n",
        "      id_vars=['model', 'config.dataset', xmetric],\n",
        "      var_name='metric',\n",
        "      value_name='value')\n",
        "\n",
        "  g = sns.FacetGrid(data=df, col='metric', sharey=False, size=5)\n",
        "  g.map_dataframe(plot_fn, x=xmetric, y='value')\n",
        "  g.set_xlabels(xlabel)\n",
        "  g.set(xscale='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBld21j5yx4I"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "186VNwRThIhE"
      },
      "outputs": [],
      "source": [
        "#@title Upstream JFT\n",
        "plot_metrics(measurements,\n",
        "             train_dataset='jft/entity:1.0.0',\n",
        "             metrics=['val_loss', 'val_prec@1', 'a/imagenet_10shot'])\n",
        "pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='jft/entity:1.0.0',\n",
        "    metrics=['val_loss', 'val_prec@1', 'a/imagenet_10shot'],\n",
        ")\n",
        "pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='jft/entity:1.0.0',\n",
        "    metrics=['val_loss', 'val_prec@1', 'a/imagenet_10shot'],\n",
        "    xmetric='tpu_days',\n",
        "    xlabel='Compute (TPUv3 core days)',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl7rOkhsuFm0"
      },
      "source": [
        "## Cifar 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzz9lwisoKL3"
      },
      "outputs": [],
      "source": [
        "#@title In-distribution\n",
        "plot_in_distribution(measurements, train_dataset='cifar10', split='test')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='cifar10',\n",
        "    metrics=['test_loss', 'test_prec@1', 'test_ece', 'test_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oote6WoS_QOd"
      },
      "outputs": [],
      "source": [
        "#@title Cifar10h\n",
        "plot_reclassified(measurements, train_dataset='cifar10')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='cifar10',\n",
        "    metrics=['cifar_10h_loss', 'cifar_10h_prec@1', 'cifar_10h_ece', 'cifar_10h_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Afa3nr-8ri"
      },
      "outputs": [],
      "source": [
        "#@title OOD\n",
        "plot_ood(measurements, train_dataset='cifar10')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ2bY0aPlQ5e"
      },
      "source": [
        "## Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q_wtXR-9CU0"
      },
      "outputs": [],
      "source": [
        "#@title In-distribution\n",
        "plot_in_distribution(measurements, train_dataset='cifar100', split='test')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='cifar100',\n",
        "    metrics=['test_loss', 'test_prec@1', 'test_ece', 'test_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j8jKADGx_ZyV"
      },
      "outputs": [],
      "source": [
        "#@title OOD\n",
        "plot_ood(measurements, train_dataset='cifar100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd4Ub0YclSmZ"
      },
      "source": [
        "## Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gokyk90V_nYw"
      },
      "outputs": [],
      "source": [
        "#@title In-distribution\n",
        "plot_in_distribution(measurements, train_dataset='imagenet2012', split='test')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='imagenet2012',\n",
        "    metrics=['test_loss', 'test_prec@1', 'test_ece', 'test_calib_auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcx57p3a_r7d"
      },
      "outputs": [],
      "source": [
        "#@title Imagenet Real\n",
        "plot_reclassified(measurements, train_dataset='imagenet2012')\n",
        "g = pareto_plot(\n",
        "    measurements,\n",
        "    train_dataset='imagenet2012',\n",
        "    metrics=[\n",
        "        'imagenet_real_loss', 'imagenet_real_prec@1', 'imagenet_real_ece',\n",
        "        'imagenet_real_calib_auc'\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRbkpaT3P3iV"
      },
      "outputs": [],
      "source": [
        "#@title ImageNet Shifts \u0026 Robustness (ImageNet-C, etc.)\n",
        "for ds in ['imagenet_c', 'imagenet_a', 'imagenet_r', 'imagenet_v2']:\n",
        "  print(ds)\n",
        "  plot_imagenet_shifts(measurements, ds)\n",
        "  plt.show()\n",
        "  pareto_plot_imagenet_shifts(measurements, ds)\n",
        "  plt.show()\n",
        "  print()\n",
        "\n",
        "for ds in ['imagenet_vid_robust', 'ytbb_robust']:\n",
        "  print(ds)\n",
        "  plot_imagenet_robustness(measurements, eval_dataset=ds)\n",
        "  plt.show()\n",
        "  pareto_plot_imagenet_robustness(measurements, ds)\n",
        "  plt.show()\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_2ETz1XVUkt"
      },
      "source": [
        "## Deep ensemble analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAn_FJHHdrmT"
      },
      "outputs": [],
      "source": [
        "def get_ensemble_scaling_measurements():\n",
        "  DE_NAMES = ['DE S/32','DE B/32','DE L/32']\n",
        "  de_measurements = get_optimal_results({\n",
        "      k: v for k, v in raw_measurements.items() if k in DE_NAMES\n",
        "  })\n",
        "\n",
        "  de_measurements = de_measurements[de_measurements['model'].isin(DE_NAMES)]\n",
        "  de_measurements['model'] = de_measurements.apply(\n",
        "      lambda x: f'{x.model}_{int(x.ensemble_size)}', axis=1)\n",
        "  de_measurements = de_measurements.drop(\n",
        "      columns=list(colab_utils.compute_metrics()), errors='ignore')\n",
        "\n",
        "  relevant_metrics = colab_utils.default_selected_metrics() + ['num_params']\n",
        "  return colab_utils.process_tuned_results(\n",
        "      de_measurements, relevant_metrics=relevant_metrics)\n",
        "\n",
        "de_results = get_ensemble_scaling_measurements()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0QTY89OdQnn"
      },
      "outputs": [],
      "source": [
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "    # 'imagenet_variants',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in colab_utils.default_fewshot_datasets()]\n",
        "\n",
        "ensemble_meas = {\n",
        "    'DE': raw_measurements['DE L/32'].query('ensemble_size==3'),\n",
        "    'Det-\u003eDE': raw_measurements['Det-\u003eDE L/32'].query('ensemble_size==3'),\n",
        "    'Det': raw_measurements['Det'],\n",
        "}\n",
        "\n",
        "ensemble_meas = get_optimal_results(ensemble_meas, verbose=False).drop(\n",
        "    columns=list(colab_utils.compute_metrics()), errors='ignore')\n",
        "df = colab_utils.process_tuned_results(ensemble_meas)\n",
        "\n",
        "display.display(\n",
        "    colab_utils.compute_score(\n",
        "        df,\n",
        "        datasets=datasets,\n",
        "        drop_1shot=True,\n",
        "        drop_incomplete_measurements=False).loc[\n",
        "            ['Det'],\n",
        "            ['score_prediction', 'score_uncertainty', 'score_adaptation']])\n",
        "\n",
        "ensemble_scores = colab_utils.compute_score(\n",
        "    df,\n",
        "    datasets=datasets,\n",
        "    drop_1shot=True,\n",
        "    drop_incomplete_measurements=False,\n",
        "    baseline_model='Det')\n",
        "ensemble_scores = ensemble_scores[[\n",
        "    'score_prediction', 'score_uncertainty', 'score_adaptation'\n",
        "]]\n",
        "\n",
        "\n",
        "def get_improvement(value):\n",
        "  improvement = (value - 1)\n",
        "  sign = '+' if improvement \u003e= 0 else '-'\n",
        "  return f'{sign}{improvement * 100:.2f}%'\n",
        "\n",
        "\n",
        "for col in ['prediction', 'uncertainty', 'adaptation']:\n",
        "  ensemble_scores[f'Rel. improvement ({col})'] = ensemble_scores[\n",
        "      f'score_{col}'].apply(get_improvement)\n",
        "\n",
        "display.display(ensemble_scores.loc[['Det-\u003eDE L/32', 'DE L/32'],\n",
        "                                    [c for c in ensemble_scores if 'Rel' in c]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMiokBFI4ZuO"
      },
      "outputs": [],
      "source": [
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "    # 'imagenet_variants',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in colab_utils.default_fewshot_datasets()]\n",
        "\n",
        "\n",
        "def plot_deep_ensemble_heatmap(scores, col_name):\n",
        "  de_scores = scores[['DE' in x and x != 'DE' for x in scores.index]]\n",
        "  de_scores.loc[:, 'model_type'] = [x[3:-2] for x in de_scores.index]\n",
        "  de_scores.loc[:, 'ensemble_size'] = [int(x[-1:]) for x in de_scores.index]\n",
        "\n",
        "  de_table = pd.pivot_table(\n",
        "      de_scores, values='score', index='model_type', columns='ensemble_size')\n",
        "  de_table = de_table.reindex(['L/32', 'B/32', 'S/32'])\n",
        "  p = sns.heatmap(de_table, annot=True, fmt='.2f', cmap='Blues', cbar=False)\n",
        "  p.set_xlabel('Ensemble Size')\n",
        "  p.set_ylabel('Model Variant')\n",
        "\n",
        "\n",
        "de_scores = colab_utils.compute_score(\n",
        "    de_results.drop(columns=['num_params']),\n",
        "    datasets=datasets,\n",
        "    drop_1shot=True,\n",
        "    drop_incomplete_measurements=True)\n",
        "de_scores = de_scores[score_cols] * 100\n",
        "\n",
        "plot_deep_ensemble_heatmap(de_scores, 'score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1eAXKK54bBn"
      },
      "outputs": [],
      "source": [
        "de_results['architecture'] = de_results.index.map(\n",
        "    lambda x: x.split(' ')[1].split('_')[0])\n",
        "de_results['ensemble_size'] = de_results.index.map(\n",
        "    lambda x: int(x.split('_')[-1]))\n",
        "\n",
        "_ = sns.scatterplot(\n",
        "    data=de_results,\n",
        "    x=('num_params', 'imagenet2012'),\n",
        "    y=('test_prec@1', 'imagenet2012'),\n",
        "    hue='architecture',\n",
        "    size='ensemble_size')\n",
        "_ = plt.ylabel('test_prec@1')\n",
        "_ = plt.xlabel('num_params')\n",
        "_ = plt.title('imagenet2012')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5klOAR4df4"
      },
      "source": [
        "# Upstream vs downstream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMO-26rn4fm1"
      },
      "outputs": [],
      "source": [
        "def get_up_vs_down_df():\n",
        "  RELEVANT_MODELS = [\n",
        "      'Det', 'BE L/32', 'Det-\u003eBE', 'DE', 'Det-\u003eDE', 'GP', 'Det-\u003eGP', 'Het',\n",
        "      'Det-\u003eHet'\n",
        "  ]\n",
        "  relevant_measurements = {\n",
        "      k: v for k, v in raw_measurements.items() if k in RELEVANT_MODELS\n",
        "  }\n",
        "  df = get_optimal_results(\n",
        "      relevant_measurements, verbose=False).drop(\n",
        "          columns=list(colab_utils.compute_metrics()), errors='ignore')\n",
        "  df = colab_utils.process_tuned_results(df)\n",
        "  return df.rename({'BE L/32': 'BE'})\n",
        "\n",
        "\n",
        "def _add_up_vs_down_metadata(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  def _adaptation_type(model_name):\n",
        "    if model_name == 'Det':\n",
        "      return 'baseline'\n",
        "    if '-\u003e' in model_name:\n",
        "      return 'down only'\n",
        "    else:\n",
        "      return 'both'\n",
        "\n",
        "  df['base_model'] = df.index.map(lambda x: x.split('-\u003e')[-1])\n",
        "  df['up_vs_down'] = df.index.map(_adaptation_type)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjQ996nE4kjJ"
      },
      "outputs": [],
      "source": [
        "def up_vs_down_plot(df, dataset, metric):\n",
        "\n",
        "  col = metric if dataset is None else (metric, dataset)\n",
        "  ymin = df[col].min()\n",
        "  ymax = df[col].max()\n",
        "\n",
        "  det_baseline = df.loc['Det', col]\n",
        "  cur_df = df[df.index != 'Det']\n",
        "\n",
        "  base_model_col = 'base_model' if dataset is None else ('base_model', '')\n",
        "  up_vs_down_col = 'up_vs_down' if dataset is None else ('up_vs_down', '')\n",
        "\n",
        "  graph = sns.barplot(\n",
        "      data=cur_df,\n",
        "      x=base_model_col,\n",
        "      y=col,\n",
        "      hue=up_vs_down_col)\n",
        "  graph.axhline(det_baseline, c='r', linewidth=2)\n",
        "  plt.ylim(max(0, ymin - .5 * (ymax - ymin)), ymax + .1 * (ymax - ymin))\n",
        "  plt.legend(title='location')\n",
        "  plt.xlabel('model')\n",
        "  plt.ylabel(metric)\n",
        "  plt.title(dataset)\n",
        "\n",
        "\n",
        "plot_datasets = ['cifar10', 'cifar100', 'imagenet2012']\n",
        "plot_metrics = ['test_prec@1', 'test_ece']\n",
        "\n",
        "df = _add_up_vs_down_metadata(get_up_vs_down_df())\n",
        "\n",
        "plt.figure(figsize=(11, 10))\n",
        "for row, dataset in enumerate(plot_datasets):\n",
        "  for col, metric in enumerate(plot_metrics):\n",
        "    index = row * len(plot_metrics) + col + 1\n",
        "    plt.subplot(len(plot_datasets), len(plot_metrics), index)\n",
        "    up_vs_down_plot(df, dataset, metric)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcTjOmEntL7o"
      },
      "outputs": [],
      "source": [
        "df = get_up_vs_down_df()\n",
        "\n",
        "# TODO(zmariet): Remove this once we have the measurements for Het and Det-\u003eHet.\n",
        "df = df.drop(columns='ood_places365_small_msp_auroc', level=0)\n",
        "\n",
        "datasets = [\n",
        "    'cifar10',\n",
        "    'cifar100',\n",
        "    'imagenet2012',\n",
        "]\n",
        "datasets += [f'few-shot {d}' for d in colab_utils.default_fewshot_datasets()]\n",
        "\n",
        "scores = colab_utils.compute_score(\n",
        "    df,\n",
        "    drop_incomplete_measurements=False,\n",
        "    drop_1shot=True,\n",
        "    datasets=datasets)\n",
        "scores = _add_up_vs_down_metadata(scores)\n",
        "\n",
        "up_vs_down_plot(scores, dataset=None, metric='score_prediction')\n",
        "plt.show()\n",
        "up_vs_down_plot(scores, dataset=None, metric='score_uncertainty')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSvqxpHL4It6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "RDL Big Paper Plots",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1QxBhHvVLapPVI0iV31WW9b3bNndiqKR5",
          "timestamp": 1652190923733
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1652124724366
        },
        {
          "file_id": "1ysMQuP_2JJNxIvGD2X3HIOyYq8PnDd0x",
          "timestamp": 1652117504457
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1652116911682
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/RDL_Big_Paper_Plots.ipynb?workspaceId=zmariet:rdl_colab::citc",
          "timestamp": 1651094726179
        },
        {
          "file_id": "17EfR-0x8-RZeRD-6_--tA3Qk1FQzriyU",
          "timestamp": 1651094693154
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1651093947882
        },
        {
          "file_id": "16tdnixI5DVYANhrkytxc549-FA5i226p",
          "timestamp": 1651008508678
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1651004739395
        },
        {
          "file_id": "16nO48cMsvHj1Yb2vBI3sym10SF4Stfsf",
          "timestamp": 1650504045971
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1650406391596
        },
        {
          "file_id": "1wVhO3x9rHqzIbCN4jd8j8PbO33iFcEf2",
          "timestamp": 1650382845945
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1650377950326
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1649944249217
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/google/colab/plots.ipynb?workspaceId=zmariet:colab::citc",
          "timestamp": 1648746918580
        },
        {
          "file_id": "1XVIrTYh6R6VpfRMHwkXF4YN6L6LO7WYh",
          "timestamp": 1648746873184
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/experimental/big_paper/plots.ipynb",
          "timestamp": 1648740356002
        },
        {
          "file_id": "1Tufx2M784xw4obIzgWXdYAX8lJGtJb3k",
          "timestamp": 1645545840302
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/google/colab/big_paper_experiments.ipynb?workspaceId=trandustin:plots::citc",
          "timestamp": 1645091101660
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/uncertainty_baselines/google/colab/big_paper_experiments.ipynb?cl=428611591",
          "timestamp": 1644888762710
        },
        {
          "file_id": "1pql3UgJFiEjGW4igFnWING7A73O_iq04",
          "timestamp": 1644878348078
        },
        {
          "file_id": "1_OgnYgLLR0zpaN2-bBQt1RE3B5JktWNN",
          "timestamp": 1643738065376
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
